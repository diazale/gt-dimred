{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import phate\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Interactive HTML tools\n",
    "from ipywidgets import interact\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.plotting import figure, show, save, output_notebook, output_file\n",
    "from bokeh.palettes import Category20b\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.palettes import PRGn\n",
    "from bokeh.palettes import Set1\n",
    "\n",
    "# Machine-learning and dimensionality reduction tools\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA as PCA # We'll use this to check our implementation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as mTSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genizon for Luke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Stockage/alex/Genizon'\n",
    "pc_file = 'Genizon_AllSamples_PCA.eigenvec'\n",
    "\n",
    "pc_path = os.path.join(data_dir, pc_file)\n",
    "\n",
    "# Import PC data. This data must be converted to an array.\n",
    "with open(pc_path) as pc:\n",
    "    pca_contents = pc.readlines()\n",
    "\n",
    "pca_data = []\n",
    "\n",
    "for pc in pca_contents:\n",
    "    pca_data.append(pc.split()[2:len(pc)])\n",
    "\n",
    "pca_data_array = np.array(pca_data).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9961, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3ababbfb5d35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmd_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_data_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GENIZON_UMAP_PC'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_NN'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_MD'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtstamp_log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \"\"\"\n\u001b[0;32m-> 1476\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1454\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m         )\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, verbose)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                                 \u001b[0mepochs_per_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                                 \u001b[0minitial_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_sample_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m                                 verbose=verbose)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "proj_dir = '/Volumes/Stockage/alex/Genizon/projections'\n",
    "\n",
    "tstamp_log = ''.join([str(t) for t in time.gmtime()[0:6]])\n",
    "nn_vals = [5,10,15,50]\n",
    "md_vals = [0.001, 0.01, 0.1, 0.5]\n",
    "pc_vals = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "\n",
    "for pc in pc_vals:\n",
    "    for nn in nn_vals:\n",
    "        for md in md_vals:\n",
    "            proj = umap.UMAP(n_components=2, n_neighbors=nn, min_dist=md).fit_transform(pca_data_array[:,:pc])\n",
    "            fname = 'GENIZON_UMAP_PC'+str(pc)+'_NN'+str(nn)+'_MD'+str(md)+'_'+tstamp_log\n",
    "            np.savetxt(os.path.join(proj_dir, fname), proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a298b8b00>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLhAIBOMMEdgFAoFgnCECu0AgEIwz/j9YSkD+BGoBRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a27eb2898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = 'GENIZON_UMAP_PC10_NN15_MD0.5_201843018199'\n",
    "\n",
    "temp_proj = np.loadtxt(os.path.join(proj_dir, fname))\n",
    "\n",
    "plt.plot(temp_proj[:,0],temp_proj[:,1],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000G data with fixed seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import values from 1000G workbook \n",
    "%store -r continents\n",
    "%store -r pop_by_continent\n",
    "%store -r pop\n",
    "%store -r indices_of_population_members\n",
    "%store -r name_by_code\n",
    "%store -r continent_by_population\n",
    "%store -r individuals\n",
    "%store -r population_by_individual\n",
    "%store -r individuals_by_population\n",
    "%store -r populations\n",
    "%store -r color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r transposed_genotype_matrix\n",
    "\n",
    "pca_full = PCA().fit(transposed_genotype_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86864"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_full.components_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.036s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 0.391980\n",
      "[t-SNE] Computed conditional probabilities in 0.088s\n",
      "[t-SNE] Iteration 50: error = 73.5225525, gradient norm = 0.0475877 (50 iterations in 3.294s)\n",
      "[t-SNE] Iteration 100: error = 62.9376984, gradient norm = 0.0183555 (50 iterations in 2.604s)\n",
      "[t-SNE] Iteration 150: error = 60.3480530, gradient norm = 0.0122277 (50 iterations in 2.457s)\n",
      "[t-SNE] Iteration 200: error = 59.1011734, gradient norm = 0.0088108 (50 iterations in 2.449s)\n",
      "[t-SNE] Iteration 250: error = 58.3405952, gradient norm = 0.0093892 (50 iterations in 2.464s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.340595\n",
      "[t-SNE] Iteration 300: error = 1.1211246, gradient norm = 0.0011570 (50 iterations in 2.772s)\n",
      "[t-SNE] Iteration 350: error = 0.7605343, gradient norm = 0.0004391 (50 iterations in 2.830s)\n",
      "[t-SNE] Iteration 400: error = 0.6289528, gradient norm = 0.0002490 (50 iterations in 2.822s)\n",
      "[t-SNE] Iteration 450: error = 0.5644455, gradient norm = 0.0001788 (50 iterations in 2.722s)\n",
      "[t-SNE] Iteration 500: error = 0.5296406, gradient norm = 0.0001418 (50 iterations in 2.654s)\n",
      "[t-SNE] Iteration 550: error = 0.5109570, gradient norm = 0.0001273 (50 iterations in 2.609s)\n",
      "[t-SNE] Iteration 600: error = 0.5001261, gradient norm = 0.0001129 (50 iterations in 2.620s)\n",
      "[t-SNE] Iteration 650: error = 0.4924490, gradient norm = 0.0001059 (50 iterations in 2.560s)\n",
      "[t-SNE] Iteration 700: error = 0.4868777, gradient norm = 0.0000965 (50 iterations in 2.542s)\n",
      "[t-SNE] Iteration 750: error = 0.4817907, gradient norm = 0.0000902 (50 iterations in 2.567s)\n",
      "[t-SNE] Iteration 800: error = 0.4773753, gradient norm = 0.0000853 (50 iterations in 2.566s)\n",
      "[t-SNE] Iteration 850: error = 0.4736841, gradient norm = 0.0000883 (50 iterations in 2.576s)\n",
      "[t-SNE] Iteration 900: error = 0.4705758, gradient norm = 0.0000754 (50 iterations in 2.573s)\n",
      "[t-SNE] Iteration 950: error = 0.4678633, gradient norm = 0.0000762 (50 iterations in 2.577s)\n",
      "[t-SNE] Iteration 1000: error = 0.4653810, gradient norm = 0.0000732 (50 iterations in 2.580s)\n",
      "[t-SNE] Error after 1000 iterations: 0.465381\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.039s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 0.649126\n",
      "[t-SNE] Computed conditional probabilities in 0.086s\n",
      "[t-SNE] Iteration 50: error = 74.0867081, gradient norm = 0.0507770 (50 iterations in 3.582s)\n",
      "[t-SNE] Iteration 100: error = 63.7455978, gradient norm = 0.0193943 (50 iterations in 2.470s)\n",
      "[t-SNE] Iteration 150: error = 61.3956375, gradient norm = 0.0133545 (50 iterations in 2.364s)\n",
      "[t-SNE] Iteration 200: error = 60.1675568, gradient norm = 0.0076397 (50 iterations in 2.347s)\n",
      "[t-SNE] Iteration 250: error = 59.4681282, gradient norm = 0.0061845 (50 iterations in 2.356s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 59.468128\n",
      "[t-SNE] Iteration 300: error = 1.2776049, gradient norm = 0.0011708 (50 iterations in 2.623s)\n",
      "[t-SNE] Iteration 350: error = 0.9270105, gradient norm = 0.0004381 (50 iterations in 2.678s)\n",
      "[t-SNE] Iteration 400: error = 0.7973257, gradient norm = 0.0002470 (50 iterations in 2.674s)\n",
      "[t-SNE] Iteration 450: error = 0.7315185, gradient norm = 0.0001760 (50 iterations in 2.656s)\n",
      "[t-SNE] Iteration 500: error = 0.6945815, gradient norm = 0.0001434 (50 iterations in 2.747s)\n",
      "[t-SNE] Iteration 550: error = 0.6742913, gradient norm = 0.0001300 (50 iterations in 2.866s)\n",
      "[t-SNE] Iteration 600: error = 0.6626050, gradient norm = 0.0001179 (50 iterations in 2.779s)\n",
      "[t-SNE] Iteration 650: error = 0.6546619, gradient norm = 0.0001052 (50 iterations in 2.688s)\n",
      "[t-SNE] Iteration 700: error = 0.6481471, gradient norm = 0.0000964 (50 iterations in 2.704s)\n",
      "[t-SNE] Iteration 750: error = 0.6424180, gradient norm = 0.0000933 (50 iterations in 2.622s)\n",
      "[t-SNE] Iteration 800: error = 0.6380458, gradient norm = 0.0000897 (50 iterations in 2.583s)\n",
      "[t-SNE] Iteration 850: error = 0.6342992, gradient norm = 0.0000818 (50 iterations in 2.814s)\n",
      "[t-SNE] Iteration 900: error = 0.6306205, gradient norm = 0.0000792 (50 iterations in 2.709s)\n",
      "[t-SNE] Iteration 950: error = 0.6275266, gradient norm = 0.0000782 (50 iterations in 2.589s)\n",
      "[t-SNE] Iteration 1000: error = 0.6248417, gradient norm = 0.0000769 (50 iterations in 2.626s)\n",
      "[t-SNE] Error after 1000 iterations: 0.624842\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.044s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 0.839793\n",
      "[t-SNE] Computed conditional probabilities in 0.085s\n",
      "[t-SNE] Iteration 50: error = 74.1223755, gradient norm = 0.0558515 (50 iterations in 3.668s)\n",
      "[t-SNE] Iteration 100: error = 63.9996834, gradient norm = 0.0198435 (50 iterations in 2.615s)\n",
      "[t-SNE] Iteration 150: error = 61.7956161, gradient norm = 0.0129740 (50 iterations in 2.485s)\n",
      "[t-SNE] Iteration 200: error = 60.7749062, gradient norm = 0.0117758 (50 iterations in 2.417s)\n",
      "[t-SNE] Iteration 250: error = 60.0490341, gradient norm = 0.0102627 (50 iterations in 2.489s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 60.049034\n",
      "[t-SNE] Iteration 300: error = 1.3507831, gradient norm = 0.0011744 (50 iterations in 2.601s)\n",
      "[t-SNE] Iteration 350: error = 1.0170939, gradient norm = 0.0004472 (50 iterations in 2.766s)\n",
      "[t-SNE] Iteration 400: error = 0.8896174, gradient norm = 0.0002607 (50 iterations in 2.846s)\n",
      "[t-SNE] Iteration 450: error = 0.8255221, gradient norm = 0.0001878 (50 iterations in 2.813s)\n",
      "[t-SNE] Iteration 500: error = 0.7906048, gradient norm = 0.0001530 (50 iterations in 2.910s)\n",
      "[t-SNE] Iteration 550: error = 0.7707851, gradient norm = 0.0001292 (50 iterations in 2.857s)\n",
      "[t-SNE] Iteration 600: error = 0.7574335, gradient norm = 0.0001123 (50 iterations in 2.802s)\n",
      "[t-SNE] Iteration 650: error = 0.7480934, gradient norm = 0.0001059 (50 iterations in 2.835s)\n",
      "[t-SNE] Iteration 700: error = 0.7412649, gradient norm = 0.0000954 (50 iterations in 2.836s)\n",
      "[t-SNE] Iteration 750: error = 0.7354253, gradient norm = 0.0000867 (50 iterations in 2.794s)\n",
      "[t-SNE] Iteration 800: error = 0.7302577, gradient norm = 0.0000898 (50 iterations in 2.744s)\n",
      "[t-SNE] Iteration 850: error = 0.7263783, gradient norm = 0.0000862 (50 iterations in 2.755s)\n",
      "[t-SNE] Iteration 900: error = 0.7232382, gradient norm = 0.0000792 (50 iterations in 2.822s)\n",
      "[t-SNE] Iteration 950: error = 0.7201716, gradient norm = 0.0000782 (50 iterations in 2.893s)\n",
      "[t-SNE] Iteration 1000: error = 0.7174929, gradient norm = 0.0000708 (50 iterations in 2.789s)\n",
      "[t-SNE] Error after 1000 iterations: 0.717493\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.051s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 1.115323\n",
      "[t-SNE] Computed conditional probabilities in 0.087s\n",
      "[t-SNE] Iteration 50: error = 73.6444702, gradient norm = 0.0552540 (50 iterations in 3.391s)\n",
      "[t-SNE] Iteration 100: error = 62.7680931, gradient norm = 0.0232162 (50 iterations in 2.605s)\n",
      "[t-SNE] Iteration 150: error = 60.2808685, gradient norm = 0.0141725 (50 iterations in 2.551s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 200: error = 59.1479492, gradient norm = 0.0099082 (50 iterations in 2.512s)\n",
      "[t-SNE] Iteration 250: error = 58.4616890, gradient norm = 0.0084526 (50 iterations in 2.332s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.461689\n",
      "[t-SNE] Iteration 300: error = 1.3227496, gradient norm = 0.0011225 (50 iterations in 2.539s)\n",
      "[t-SNE] Iteration 350: error = 1.0075876, gradient norm = 0.0004276 (50 iterations in 2.644s)\n",
      "[t-SNE] Iteration 400: error = 0.8835937, gradient norm = 0.0002500 (50 iterations in 2.612s)\n",
      "[t-SNE] Iteration 450: error = 0.8206481, gradient norm = 0.0001844 (50 iterations in 2.593s)\n",
      "[t-SNE] Iteration 500: error = 0.7864810, gradient norm = 0.0001489 (50 iterations in 2.597s)\n",
      "[t-SNE] Iteration 550: error = 0.7666804, gradient norm = 0.0001298 (50 iterations in 2.600s)\n",
      "[t-SNE] Iteration 600: error = 0.7539644, gradient norm = 0.0001191 (50 iterations in 2.598s)\n",
      "[t-SNE] Iteration 650: error = 0.7452162, gradient norm = 0.0001077 (50 iterations in 2.596s)\n",
      "[t-SNE] Iteration 700: error = 0.7382386, gradient norm = 0.0001054 (50 iterations in 2.598s)\n",
      "[t-SNE] Iteration 750: error = 0.7330288, gradient norm = 0.0001000 (50 iterations in 2.625s)\n",
      "[t-SNE] Iteration 800: error = 0.7291861, gradient norm = 0.0000950 (50 iterations in 2.691s)\n",
      "[t-SNE] Iteration 850: error = 0.7259413, gradient norm = 0.0000938 (50 iterations in 2.706s)\n",
      "[t-SNE] Iteration 900: error = 0.7234139, gradient norm = 0.0000829 (50 iterations in 2.739s)\n",
      "[t-SNE] Iteration 950: error = 0.7203221, gradient norm = 0.0000807 (50 iterations in 2.719s)\n",
      "[t-SNE] Iteration 1000: error = 0.7171178, gradient norm = 0.0000772 (50 iterations in 2.744s)\n",
      "[t-SNE] Error after 1000 iterations: 0.717118\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.049s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 1.328850\n",
      "[t-SNE] Computed conditional probabilities in 0.084s\n",
      "[t-SNE] Iteration 50: error = 73.8985825, gradient norm = 0.0502703 (50 iterations in 3.353s)\n",
      "[t-SNE] Iteration 100: error = 62.0793381, gradient norm = 0.0219620 (50 iterations in 2.543s)\n",
      "[t-SNE] Iteration 150: error = 59.3177490, gradient norm = 0.0166560 (50 iterations in 2.471s)\n",
      "[t-SNE] Iteration 200: error = 58.0359077, gradient norm = 0.0100834 (50 iterations in 2.402s)\n",
      "[t-SNE] Iteration 250: error = 57.2962379, gradient norm = 0.0091377 (50 iterations in 2.395s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.296238\n",
      "[t-SNE] Iteration 300: error = 1.3089148, gradient norm = 0.0010840 (50 iterations in 2.592s)\n",
      "[t-SNE] Iteration 350: error = 1.0017433, gradient norm = 0.0004210 (50 iterations in 2.671s)\n",
      "[t-SNE] Iteration 400: error = 0.8809171, gradient norm = 0.0002499 (50 iterations in 2.690s)\n",
      "[t-SNE] Iteration 450: error = 0.8193395, gradient norm = 0.0001840 (50 iterations in 2.655s)\n",
      "[t-SNE] Iteration 500: error = 0.7853923, gradient norm = 0.0001502 (50 iterations in 2.668s)\n",
      "[t-SNE] Iteration 550: error = 0.7664745, gradient norm = 0.0001372 (50 iterations in 2.584s)\n",
      "[t-SNE] Iteration 600: error = 0.7554652, gradient norm = 0.0001247 (50 iterations in 2.606s)\n",
      "[t-SNE] Iteration 650: error = 0.7472260, gradient norm = 0.0001101 (50 iterations in 2.588s)\n",
      "[t-SNE] Iteration 700: error = 0.7402989, gradient norm = 0.0001001 (50 iterations in 2.588s)\n",
      "[t-SNE] Iteration 750: error = 0.7343811, gradient norm = 0.0000934 (50 iterations in 2.606s)\n",
      "[t-SNE] Iteration 800: error = 0.7295079, gradient norm = 0.0000909 (50 iterations in 2.578s)\n",
      "[t-SNE] Iteration 850: error = 0.7252120, gradient norm = 0.0000836 (50 iterations in 2.590s)\n",
      "[t-SNE] Iteration 900: error = 0.7211626, gradient norm = 0.0000873 (50 iterations in 2.604s)\n",
      "[t-SNE] Iteration 950: error = 0.7176666, gradient norm = 0.0000889 (50 iterations in 2.602s)\n",
      "[t-SNE] Iteration 1000: error = 0.7148420, gradient norm = 0.0000813 (50 iterations in 2.575s)\n",
      "[t-SNE] Error after 1000 iterations: 0.714842\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.050s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 1.513589\n",
      "[t-SNE] Computed conditional probabilities in 0.085s\n",
      "[t-SNE] Iteration 50: error = 73.3950958, gradient norm = 0.0557087 (50 iterations in 3.412s)\n",
      "[t-SNE] Iteration 100: error = 61.5670624, gradient norm = 0.0266981 (50 iterations in 2.555s)\n",
      "[t-SNE] Iteration 150: error = 58.8887177, gradient norm = 0.0234481 (50 iterations in 2.489s)\n",
      "[t-SNE] Iteration 200: error = 57.6353989, gradient norm = 0.0156528 (50 iterations in 2.427s)\n",
      "[t-SNE] Iteration 250: error = 56.9012756, gradient norm = 0.0138464 (50 iterations in 2.402s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.901276\n",
      "[t-SNE] Iteration 300: error = 1.3339719, gradient norm = 0.0011543 (50 iterations in 2.602s)\n",
      "[t-SNE] Iteration 350: error = 1.0186481, gradient norm = 0.0005135 (50 iterations in 2.679s)\n",
      "[t-SNE] Iteration 400: error = 0.8942607, gradient norm = 0.0002554 (50 iterations in 2.703s)\n",
      "[t-SNE] Iteration 450: error = 0.8311312, gradient norm = 0.0001866 (50 iterations in 2.619s)\n",
      "[t-SNE] Iteration 500: error = 0.7958969, gradient norm = 0.0001550 (50 iterations in 2.601s)\n",
      "[t-SNE] Iteration 550: error = 0.7758222, gradient norm = 0.0001339 (50 iterations in 2.617s)\n",
      "[t-SNE] Iteration 600: error = 0.7634625, gradient norm = 0.0001196 (50 iterations in 2.589s)\n",
      "[t-SNE] Iteration 650: error = 0.7542663, gradient norm = 0.0001057 (50 iterations in 2.637s)\n",
      "[t-SNE] Iteration 700: error = 0.7465754, gradient norm = 0.0001037 (50 iterations in 2.610s)\n",
      "[t-SNE] Iteration 750: error = 0.7403969, gradient norm = 0.0001036 (50 iterations in 2.689s)\n",
      "[t-SNE] Iteration 800: error = 0.7355410, gradient norm = 0.0000979 (50 iterations in 2.632s)\n",
      "[t-SNE] Iteration 850: error = 0.7320214, gradient norm = 0.0000936 (50 iterations in 2.588s)\n",
      "[t-SNE] Iteration 900: error = 0.7290330, gradient norm = 0.0000829 (50 iterations in 2.581s)\n",
      "[t-SNE] Iteration 950: error = 0.7260754, gradient norm = 0.0000876 (50 iterations in 2.581s)\n",
      "[t-SNE] Iteration 1000: error = 0.7232060, gradient norm = 0.0000846 (50 iterations in 2.569s)\n",
      "[t-SNE] Error after 1000 iterations: 0.723206\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.054s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 1.750166\n",
      "[t-SNE] Computed conditional probabilities in 0.086s\n",
      "[t-SNE] Iteration 50: error = 73.3751068, gradient norm = 0.0562986 (50 iterations in 3.372s)\n",
      "[t-SNE] Iteration 100: error = 61.4139061, gradient norm = 0.0248809 (50 iterations in 2.491s)\n",
      "[t-SNE] Iteration 150: error = 58.5204315, gradient norm = 0.0181276 (50 iterations in 2.458s)\n",
      "[t-SNE] Iteration 200: error = 57.2109489, gradient norm = 0.0130858 (50 iterations in 2.433s)\n",
      "[t-SNE] Iteration 250: error = 56.4302635, gradient norm = 0.0134064 (50 iterations in 2.393s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.430264\n",
      "[t-SNE] Iteration 300: error = 1.2953218, gradient norm = 0.0011314 (50 iterations in 2.612s)\n",
      "[t-SNE] Iteration 350: error = 0.9864445, gradient norm = 0.0004258 (50 iterations in 2.674s)\n",
      "[t-SNE] Iteration 400: error = 0.8649244, gradient norm = 0.0002498 (50 iterations in 2.682s)\n",
      "[t-SNE] Iteration 450: error = 0.8040639, gradient norm = 0.0001862 (50 iterations in 2.722s)\n",
      "[t-SNE] Iteration 500: error = 0.7711987, gradient norm = 0.0001605 (50 iterations in 2.617s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 550: error = 0.7529510, gradient norm = 0.0001379 (50 iterations in 2.679s)\n",
      "[t-SNE] Iteration 600: error = 0.7412755, gradient norm = 0.0001252 (50 iterations in 2.744s)\n",
      "[t-SNE] Iteration 650: error = 0.7327899, gradient norm = 0.0001186 (50 iterations in 2.694s)\n",
      "[t-SNE] Iteration 700: error = 0.7263613, gradient norm = 0.0001124 (50 iterations in 2.701s)\n",
      "[t-SNE] Iteration 750: error = 0.7207277, gradient norm = 0.0001078 (50 iterations in 2.668s)\n",
      "[t-SNE] Iteration 800: error = 0.7161691, gradient norm = 0.0000955 (50 iterations in 2.619s)\n",
      "[t-SNE] Iteration 850: error = 0.7118413, gradient norm = 0.0000929 (50 iterations in 2.561s)\n",
      "[t-SNE] Iteration 900: error = 0.7076909, gradient norm = 0.0000899 (50 iterations in 2.641s)\n",
      "[t-SNE] Iteration 950: error = 0.7041150, gradient norm = 0.0000880 (50 iterations in 2.539s)\n",
      "[t-SNE] Iteration 1000: error = 0.7010613, gradient norm = 0.0000991 (50 iterations in 2.544s)\n",
      "[t-SNE] Error after 1000 iterations: 0.701061\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.060s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 2.167910\n",
      "[t-SNE] Computed conditional probabilities in 0.089s\n",
      "[t-SNE] Iteration 50: error = 73.8837662, gradient norm = 0.0659821 (50 iterations in 3.399s)\n",
      "[t-SNE] Iteration 100: error = 61.7203102, gradient norm = 0.0251262 (50 iterations in 2.573s)\n",
      "[t-SNE] Iteration 150: error = 58.9403534, gradient norm = 0.0156161 (50 iterations in 2.501s)\n",
      "[t-SNE] Iteration 200: error = 57.6579628, gradient norm = 0.0154301 (50 iterations in 2.483s)\n",
      "[t-SNE] Iteration 250: error = 56.8997307, gradient norm = 0.0097475 (50 iterations in 2.463s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.899731\n",
      "[t-SNE] Iteration 300: error = 1.2554430, gradient norm = 0.0011378 (50 iterations in 2.592s)\n",
      "[t-SNE] Iteration 350: error = 0.9358187, gradient norm = 0.0004279 (50 iterations in 2.609s)\n",
      "[t-SNE] Iteration 400: error = 0.8129892, gradient norm = 0.0002552 (50 iterations in 2.602s)\n",
      "[t-SNE] Iteration 450: error = 0.7510626, gradient norm = 0.0001847 (50 iterations in 2.580s)\n",
      "[t-SNE] Iteration 500: error = 0.7172741, gradient norm = 0.0001481 (50 iterations in 2.558s)\n",
      "[t-SNE] Iteration 550: error = 0.6979319, gradient norm = 0.0001294 (50 iterations in 2.542s)\n",
      "[t-SNE] Iteration 600: error = 0.6856345, gradient norm = 0.0001241 (50 iterations in 2.556s)\n",
      "[t-SNE] Iteration 650: error = 0.6775407, gradient norm = 0.0001105 (50 iterations in 2.638s)\n",
      "[t-SNE] Iteration 700: error = 0.6713863, gradient norm = 0.0001078 (50 iterations in 2.552s)\n",
      "[t-SNE] Iteration 750: error = 0.6663808, gradient norm = 0.0000926 (50 iterations in 2.554s)\n",
      "[t-SNE] Iteration 800: error = 0.6618233, gradient norm = 0.0000969 (50 iterations in 2.531s)\n",
      "[t-SNE] Iteration 850: error = 0.6579325, gradient norm = 0.0000923 (50 iterations in 2.539s)\n",
      "[t-SNE] Iteration 900: error = 0.6544932, gradient norm = 0.0000877 (50 iterations in 2.548s)\n",
      "[t-SNE] Iteration 950: error = 0.6514996, gradient norm = 0.0000843 (50 iterations in 2.555s)\n",
      "[t-SNE] Iteration 1000: error = 0.6487756, gradient norm = 0.0000796 (50 iterations in 2.566s)\n",
      "[t-SNE] Error after 1000 iterations: 0.648776\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.086s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 2.303880\n",
      "[t-SNE] Computed conditional probabilities in 0.093s\n",
      "[t-SNE] Iteration 50: error = 73.9280701, gradient norm = 0.0637429 (50 iterations in 3.624s)\n",
      "[t-SNE] Iteration 100: error = 61.7588882, gradient norm = 0.0293917 (50 iterations in 2.559s)\n",
      "[t-SNE] Iteration 150: error = 59.0628624, gradient norm = 0.0175249 (50 iterations in 2.449s)\n",
      "[t-SNE] Iteration 200: error = 57.7884598, gradient norm = 0.0187430 (50 iterations in 2.419s)\n",
      "[t-SNE] Iteration 250: error = 57.0323067, gradient norm = 0.0129070 (50 iterations in 2.369s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.032307\n",
      "[t-SNE] Iteration 300: error = 1.2775384, gradient norm = 0.0011178 (50 iterations in 2.652s)\n",
      "[t-SNE] Iteration 350: error = 0.9612315, gradient norm = 0.0004237 (50 iterations in 2.694s)\n",
      "[t-SNE] Iteration 400: error = 0.8397343, gradient norm = 0.0002494 (50 iterations in 2.662s)\n",
      "[t-SNE] Iteration 450: error = 0.7785237, gradient norm = 0.0001817 (50 iterations in 2.629s)\n",
      "[t-SNE] Iteration 500: error = 0.7450992, gradient norm = 0.0001448 (50 iterations in 2.609s)\n",
      "[t-SNE] Iteration 550: error = 0.7243531, gradient norm = 0.0001330 (50 iterations in 2.631s)\n",
      "[t-SNE] Iteration 600: error = 0.7112547, gradient norm = 0.0001130 (50 iterations in 2.601s)\n",
      "[t-SNE] Iteration 650: error = 0.7023366, gradient norm = 0.0001089 (50 iterations in 2.582s)\n",
      "[t-SNE] Iteration 700: error = 0.6954525, gradient norm = 0.0000984 (50 iterations in 2.668s)\n",
      "[t-SNE] Iteration 750: error = 0.6899719, gradient norm = 0.0001047 (50 iterations in 2.647s)\n",
      "[t-SNE] Iteration 800: error = 0.6863675, gradient norm = 0.0000884 (50 iterations in 2.646s)\n",
      "[t-SNE] Iteration 850: error = 0.6826686, gradient norm = 0.0000833 (50 iterations in 2.627s)\n",
      "[t-SNE] Iteration 900: error = 0.6789378, gradient norm = 0.0000853 (50 iterations in 2.580s)\n",
      "[t-SNE] Iteration 950: error = 0.6759498, gradient norm = 0.0000901 (50 iterations in 2.582s)\n",
      "[t-SNE] Iteration 1000: error = 0.6737601, gradient norm = 0.0000756 (50 iterations in 2.580s)\n",
      "[t-SNE] Error after 1000 iterations: 0.673760\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.086s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 2.547385\n",
      "[t-SNE] Computed conditional probabilities in 0.091s\n",
      "[t-SNE] Iteration 50: error = 74.1196747, gradient norm = 0.0600844 (50 iterations in 3.505s)\n",
      "[t-SNE] Iteration 100: error = 61.6515503, gradient norm = 0.0254499 (50 iterations in 2.544s)\n",
      "[t-SNE] Iteration 150: error = 58.6850891, gradient norm = 0.0227322 (50 iterations in 2.457s)\n",
      "[t-SNE] Iteration 200: error = 57.2983360, gradient norm = 0.0129587 (50 iterations in 2.419s)\n",
      "[t-SNE] Iteration 250: error = 56.4911156, gradient norm = 0.0119009 (50 iterations in 2.406s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.491116\n",
      "[t-SNE] Iteration 300: error = 1.2538416, gradient norm = 0.0011318 (50 iterations in 2.626s)\n",
      "[t-SNE] Iteration 350: error = 0.9340056, gradient norm = 0.0004312 (50 iterations in 2.680s)\n",
      "[t-SNE] Iteration 400: error = 0.8111196, gradient norm = 0.0002504 (50 iterations in 2.618s)\n",
      "[t-SNE] Iteration 450: error = 0.7487728, gradient norm = 0.0001833 (50 iterations in 2.608s)\n",
      "[t-SNE] Iteration 500: error = 0.7140920, gradient norm = 0.0001468 (50 iterations in 2.618s)\n",
      "[t-SNE] Iteration 550: error = 0.6940261, gradient norm = 0.0001343 (50 iterations in 2.639s)\n",
      "[t-SNE] Iteration 600: error = 0.6819784, gradient norm = 0.0001321 (50 iterations in 2.601s)\n",
      "[t-SNE] Iteration 650: error = 0.6745341, gradient norm = 0.0001194 (50 iterations in 2.650s)\n",
      "[t-SNE] Iteration 700: error = 0.6689668, gradient norm = 0.0001203 (50 iterations in 2.583s)\n",
      "[t-SNE] Iteration 750: error = 0.6647813, gradient norm = 0.0000995 (50 iterations in 2.659s)\n",
      "[t-SNE] Iteration 800: error = 0.6600946, gradient norm = 0.0000962 (50 iterations in 2.662s)\n",
      "[t-SNE] Iteration 850: error = 0.6559474, gradient norm = 0.0000902 (50 iterations in 2.692s)\n",
      "[t-SNE] Iteration 900: error = 0.6524380, gradient norm = 0.0000794 (50 iterations in 2.668s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 950: error = 0.6490058, gradient norm = 0.0000852 (50 iterations in 2.599s)\n",
      "[t-SNE] Iteration 1000: error = 0.6460459, gradient norm = 0.0000786 (50 iterations in 2.578s)\n",
      "[t-SNE] Error after 1000 iterations: 0.646046\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.093s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 2.703332\n",
      "[t-SNE] Computed conditional probabilities in 0.092s\n",
      "[t-SNE] Iteration 50: error = 74.3129959, gradient norm = 0.0659188 (50 iterations in 3.528s)\n",
      "[t-SNE] Iteration 100: error = 61.7647018, gradient norm = 0.0277794 (50 iterations in 2.528s)\n",
      "[t-SNE] Iteration 150: error = 58.8040848, gradient norm = 0.0167553 (50 iterations in 2.422s)\n",
      "[t-SNE] Iteration 200: error = 57.4112358, gradient norm = 0.0151294 (50 iterations in 2.395s)\n",
      "[t-SNE] Iteration 250: error = 56.5863457, gradient norm = 0.0111179 (50 iterations in 2.381s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.586346\n",
      "[t-SNE] Iteration 300: error = 1.2813103, gradient norm = 0.0011733 (50 iterations in 2.618s)\n",
      "[t-SNE] Iteration 350: error = 0.9593069, gradient norm = 0.0004332 (50 iterations in 2.621s)\n",
      "[t-SNE] Iteration 400: error = 0.8338414, gradient norm = 0.0002568 (50 iterations in 2.675s)\n",
      "[t-SNE] Iteration 450: error = 0.7707535, gradient norm = 0.0001835 (50 iterations in 2.610s)\n",
      "[t-SNE] Iteration 500: error = 0.7351542, gradient norm = 0.0001472 (50 iterations in 2.612s)\n",
      "[t-SNE] Iteration 550: error = 0.7136910, gradient norm = 0.0001318 (50 iterations in 2.630s)\n",
      "[t-SNE] Iteration 600: error = 0.7004733, gradient norm = 0.0001205 (50 iterations in 2.602s)\n",
      "[t-SNE] Iteration 650: error = 0.6918125, gradient norm = 0.0001128 (50 iterations in 2.566s)\n",
      "[t-SNE] Iteration 700: error = 0.6859379, gradient norm = 0.0001054 (50 iterations in 2.633s)\n",
      "[t-SNE] Iteration 750: error = 0.6803666, gradient norm = 0.0000964 (50 iterations in 2.643s)\n",
      "[t-SNE] Iteration 800: error = 0.6753979, gradient norm = 0.0000969 (50 iterations in 2.583s)\n",
      "[t-SNE] Iteration 850: error = 0.6715681, gradient norm = 0.0000906 (50 iterations in 2.568s)\n",
      "[t-SNE] Iteration 900: error = 0.6682743, gradient norm = 0.0000830 (50 iterations in 2.605s)\n",
      "[t-SNE] Iteration 950: error = 0.6650578, gradient norm = 0.0000807 (50 iterations in 2.666s)\n",
      "[t-SNE] Iteration 1000: error = 0.6615189, gradient norm = 0.0000776 (50 iterations in 2.663s)\n",
      "[t-SNE] Error after 1000 iterations: 0.661519\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.097s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 2.857593\n",
      "[t-SNE] Computed conditional probabilities in 0.091s\n",
      "[t-SNE] Iteration 50: error = 74.0546112, gradient norm = 0.0630093 (50 iterations in 3.630s)\n",
      "[t-SNE] Iteration 100: error = 61.3934746, gradient norm = 0.0262736 (50 iterations in 2.561s)\n",
      "[t-SNE] Iteration 150: error = 58.4877510, gradient norm = 0.0166920 (50 iterations in 2.465s)\n",
      "[t-SNE] Iteration 200: error = 57.1360283, gradient norm = 0.0188947 (50 iterations in 2.431s)\n",
      "[t-SNE] Iteration 250: error = 56.3357620, gradient norm = 0.0107883 (50 iterations in 2.419s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.335762\n",
      "[t-SNE] Iteration 300: error = 1.2781658, gradient norm = 0.0011330 (50 iterations in 2.552s)\n",
      "[t-SNE] Iteration 350: error = 0.9644008, gradient norm = 0.0004295 (50 iterations in 2.638s)\n",
      "[t-SNE] Iteration 400: error = 0.8417299, gradient norm = 0.0002541 (50 iterations in 2.652s)\n",
      "[t-SNE] Iteration 450: error = 0.7801549, gradient norm = 0.0001854 (50 iterations in 2.612s)\n",
      "[t-SNE] Iteration 500: error = 0.7465577, gradient norm = 0.0001491 (50 iterations in 2.602s)\n",
      "[t-SNE] Iteration 550: error = 0.7263394, gradient norm = 0.0001388 (50 iterations in 2.607s)\n",
      "[t-SNE] Iteration 600: error = 0.7140357, gradient norm = 0.0001210 (50 iterations in 2.582s)\n",
      "[t-SNE] Iteration 650: error = 0.7049946, gradient norm = 0.0001153 (50 iterations in 2.577s)\n",
      "[t-SNE] Iteration 700: error = 0.6982098, gradient norm = 0.0001116 (50 iterations in 2.559s)\n",
      "[t-SNE] Iteration 750: error = 0.6931871, gradient norm = 0.0001006 (50 iterations in 2.557s)\n",
      "[t-SNE] Iteration 800: error = 0.6888534, gradient norm = 0.0000943 (50 iterations in 2.560s)\n",
      "[t-SNE] Iteration 850: error = 0.6849363, gradient norm = 0.0000913 (50 iterations in 2.538s)\n",
      "[t-SNE] Iteration 900: error = 0.6814381, gradient norm = 0.0000909 (50 iterations in 2.524s)\n",
      "[t-SNE] Iteration 950: error = 0.6783174, gradient norm = 0.0000938 (50 iterations in 2.539s)\n",
      "[t-SNE] Iteration 1000: error = 0.6757753, gradient norm = 0.0000820 (50 iterations in 2.543s)\n",
      "[t-SNE] Error after 1000 iterations: 0.675775\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.100s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 2.982772\n",
      "[t-SNE] Computed conditional probabilities in 0.092s\n",
      "[t-SNE] Iteration 50: error = 74.4366531, gradient norm = 0.0671904 (50 iterations in 3.582s)\n",
      "[t-SNE] Iteration 100: error = 61.3291817, gradient norm = 0.0264008 (50 iterations in 2.565s)\n",
      "[t-SNE] Iteration 150: error = 58.4147339, gradient norm = 0.0144338 (50 iterations in 2.477s)\n",
      "[t-SNE] Iteration 200: error = 57.0536652, gradient norm = 0.0125666 (50 iterations in 2.447s)\n",
      "[t-SNE] Iteration 250: error = 56.2544632, gradient norm = 0.0105377 (50 iterations in 2.440s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.254463\n",
      "[t-SNE] Iteration 300: error = 1.2927512, gradient norm = 0.0011200 (50 iterations in 2.601s)\n",
      "[t-SNE] Iteration 350: error = 0.9825472, gradient norm = 0.0004296 (50 iterations in 2.645s)\n",
      "[t-SNE] Iteration 400: error = 0.8603814, gradient norm = 0.0002555 (50 iterations in 2.647s)\n",
      "[t-SNE] Iteration 450: error = 0.7980478, gradient norm = 0.0001857 (50 iterations in 2.632s)\n",
      "[t-SNE] Iteration 500: error = 0.7639071, gradient norm = 0.0001515 (50 iterations in 2.626s)\n",
      "[t-SNE] Iteration 550: error = 0.7436004, gradient norm = 0.0001359 (50 iterations in 2.627s)\n",
      "[t-SNE] Iteration 600: error = 0.7298302, gradient norm = 0.0001231 (50 iterations in 2.624s)\n",
      "[t-SNE] Iteration 650: error = 0.7209018, gradient norm = 0.0001210 (50 iterations in 2.642s)\n",
      "[t-SNE] Iteration 700: error = 0.7148817, gradient norm = 0.0001070 (50 iterations in 2.669s)\n",
      "[t-SNE] Iteration 750: error = 0.7094036, gradient norm = 0.0000989 (50 iterations in 2.669s)\n",
      "[t-SNE] Iteration 800: error = 0.7049415, gradient norm = 0.0000987 (50 iterations in 2.640s)\n",
      "[t-SNE] Iteration 850: error = 0.7009507, gradient norm = 0.0000847 (50 iterations in 2.639s)\n",
      "[t-SNE] Iteration 900: error = 0.6968427, gradient norm = 0.0000801 (50 iterations in 2.704s)\n",
      "[t-SNE] Iteration 950: error = 0.6924949, gradient norm = 0.0000784 (50 iterations in 2.659s)\n",
      "[t-SNE] Iteration 1000: error = 0.6882671, gradient norm = 0.0000784 (50 iterations in 2.650s)\n",
      "[t-SNE] Error after 1000 iterations: 0.688267\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.104s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 3.124026\n",
      "[t-SNE] Computed conditional probabilities in 0.094s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 50: error = 74.2029572, gradient norm = 0.0630584 (50 iterations in 3.623s)\n",
      "[t-SNE] Iteration 100: error = 61.3280067, gradient norm = 0.0285648 (50 iterations in 2.743s)\n",
      "[t-SNE] Iteration 150: error = 58.4038849, gradient norm = 0.0178174 (50 iterations in 2.484s)\n",
      "[t-SNE] Iteration 200: error = 57.0685005, gradient norm = 0.0163803 (50 iterations in 2.470s)\n",
      "[t-SNE] Iteration 250: error = 56.2588921, gradient norm = 0.0170249 (50 iterations in 2.407s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.258892\n",
      "[t-SNE] Iteration 300: error = 1.2910784, gradient norm = 0.0011177 (50 iterations in 2.591s)\n",
      "[t-SNE] Iteration 350: error = 0.9814227, gradient norm = 0.0004292 (50 iterations in 2.698s)\n",
      "[t-SNE] Iteration 400: error = 0.8595857, gradient norm = 0.0002566 (50 iterations in 2.645s)\n",
      "[t-SNE] Iteration 450: error = 0.7987397, gradient norm = 0.0001865 (50 iterations in 2.680s)\n",
      "[t-SNE] Iteration 500: error = 0.7649862, gradient norm = 0.0001564 (50 iterations in 2.616s)\n",
      "[t-SNE] Iteration 550: error = 0.7444065, gradient norm = 0.0001351 (50 iterations in 2.676s)\n",
      "[t-SNE] Iteration 600: error = 0.7307006, gradient norm = 0.0001200 (50 iterations in 2.706s)\n",
      "[t-SNE] Iteration 650: error = 0.7216666, gradient norm = 0.0001144 (50 iterations in 2.707s)\n",
      "[t-SNE] Iteration 700: error = 0.7151054, gradient norm = 0.0001125 (50 iterations in 2.704s)\n",
      "[t-SNE] Iteration 750: error = 0.7095327, gradient norm = 0.0001045 (50 iterations in 2.664s)\n",
      "[t-SNE] Iteration 800: error = 0.7049418, gradient norm = 0.0000949 (50 iterations in 2.660s)\n",
      "[t-SNE] Iteration 850: error = 0.7011664, gradient norm = 0.0001063 (50 iterations in 2.723s)\n",
      "[t-SNE] Iteration 900: error = 0.6982419, gradient norm = 0.0000922 (50 iterations in 2.679s)\n",
      "[t-SNE] Iteration 950: error = 0.6953912, gradient norm = 0.0000883 (50 iterations in 2.682s)\n",
      "[t-SNE] Iteration 1000: error = 0.6929551, gradient norm = 0.0000821 (50 iterations in 2.698s)\n",
      "[t-SNE] Error after 1000 iterations: 0.692955\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.118s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 3.245173\n",
      "[t-SNE] Computed conditional probabilities in 0.097s\n",
      "[t-SNE] Iteration 50: error = 74.4668884, gradient norm = 0.0677140 (50 iterations in 4.095s)\n",
      "[t-SNE] Iteration 100: error = 61.4571533, gradient norm = 0.0276485 (50 iterations in 2.642s)\n",
      "[t-SNE] Iteration 150: error = 58.4934654, gradient norm = 0.0197107 (50 iterations in 2.461s)\n",
      "[t-SNE] Iteration 200: error = 57.1474876, gradient norm = 0.0172828 (50 iterations in 2.420s)\n",
      "[t-SNE] Iteration 250: error = 56.3440666, gradient norm = 0.0122089 (50 iterations in 2.409s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.344067\n",
      "[t-SNE] Iteration 300: error = 1.3177687, gradient norm = 0.0011290 (50 iterations in 2.592s)\n",
      "[t-SNE] Iteration 350: error = 1.0053487, gradient norm = 0.0004294 (50 iterations in 2.661s)\n",
      "[t-SNE] Iteration 400: error = 0.8839032, gradient norm = 0.0002565 (50 iterations in 2.613s)\n",
      "[t-SNE] Iteration 450: error = 0.8227301, gradient norm = 0.0001924 (50 iterations in 2.599s)\n",
      "[t-SNE] Iteration 500: error = 0.7888744, gradient norm = 0.0001525 (50 iterations in 2.617s)\n",
      "[t-SNE] Iteration 550: error = 0.7681258, gradient norm = 0.0001496 (50 iterations in 2.597s)\n",
      "[t-SNE] Iteration 600: error = 0.7545412, gradient norm = 0.0001200 (50 iterations in 2.595s)\n",
      "[t-SNE] Iteration 650: error = 0.7446125, gradient norm = 0.0001138 (50 iterations in 2.607s)\n",
      "[t-SNE] Iteration 700: error = 0.7372932, gradient norm = 0.0001089 (50 iterations in 2.600s)\n",
      "[t-SNE] Iteration 750: error = 0.7322206, gradient norm = 0.0001073 (50 iterations in 2.592s)\n",
      "[t-SNE] Iteration 800: error = 0.7284076, gradient norm = 0.0000920 (50 iterations in 2.606s)\n",
      "[t-SNE] Iteration 850: error = 0.7243518, gradient norm = 0.0001052 (50 iterations in 2.612s)\n",
      "[t-SNE] Iteration 900: error = 0.7208964, gradient norm = 0.0000989 (50 iterations in 2.609s)\n",
      "[t-SNE] Iteration 950: error = 0.7181457, gradient norm = 0.0000878 (50 iterations in 2.603s)\n",
      "[t-SNE] Iteration 1000: error = 0.7155331, gradient norm = 0.0000856 (50 iterations in 2.609s)\n",
      "[t-SNE] Error after 1000 iterations: 0.715533\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.116s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 3.381779\n",
      "[t-SNE] Computed conditional probabilities in 0.095s\n",
      "[t-SNE] Iteration 50: error = 74.6137009, gradient norm = 0.0670788 (50 iterations in 3.709s)\n",
      "[t-SNE] Iteration 100: error = 61.4312019, gradient norm = 0.0242442 (50 iterations in 2.587s)\n",
      "[t-SNE] Iteration 150: error = 58.5018883, gradient norm = 0.0193892 (50 iterations in 2.493s)\n",
      "[t-SNE] Iteration 200: error = 57.1618729, gradient norm = 0.0111222 (50 iterations in 2.472s)\n",
      "[t-SNE] Iteration 250: error = 56.3599854, gradient norm = 0.0099526 (50 iterations in 2.493s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.359985\n",
      "[t-SNE] Iteration 300: error = 1.3143605, gradient norm = 0.0011377 (50 iterations in 2.615s)\n",
      "[t-SNE] Iteration 350: error = 1.0042261, gradient norm = 0.0004312 (50 iterations in 2.648s)\n",
      "[t-SNE] Iteration 400: error = 0.8816713, gradient norm = 0.0002594 (50 iterations in 2.665s)\n",
      "[t-SNE] Iteration 450: error = 0.8204809, gradient norm = 0.0001932 (50 iterations in 2.661s)\n",
      "[t-SNE] Iteration 500: error = 0.7863972, gradient norm = 0.0001586 (50 iterations in 2.711s)\n",
      "[t-SNE] Iteration 550: error = 0.7665791, gradient norm = 0.0001399 (50 iterations in 2.620s)\n",
      "[t-SNE] Iteration 600: error = 0.7530865, gradient norm = 0.0001247 (50 iterations in 2.661s)\n",
      "[t-SNE] Iteration 650: error = 0.7434688, gradient norm = 0.0001214 (50 iterations in 2.645s)\n",
      "[t-SNE] Iteration 700: error = 0.7368438, gradient norm = 0.0001041 (50 iterations in 2.642s)\n",
      "[t-SNE] Iteration 750: error = 0.7307295, gradient norm = 0.0000952 (50 iterations in 2.655s)\n",
      "[t-SNE] Iteration 800: error = 0.7246591, gradient norm = 0.0000911 (50 iterations in 2.698s)\n",
      "[t-SNE] Iteration 850: error = 0.7195955, gradient norm = 0.0000908 (50 iterations in 2.637s)\n",
      "[t-SNE] Iteration 900: error = 0.7154058, gradient norm = 0.0000840 (50 iterations in 2.658s)\n",
      "[t-SNE] Iteration 950: error = 0.7117069, gradient norm = 0.0000819 (50 iterations in 2.690s)\n",
      "[t-SNE] Iteration 1000: error = 0.7080539, gradient norm = 0.0000815 (50 iterations in 2.712s)\n",
      "[t-SNE] Error after 1000 iterations: 0.708054\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.126s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 3.529666\n",
      "[t-SNE] Computed conditional probabilities in 0.096s\n",
      "[t-SNE] Iteration 50: error = 74.2419281, gradient norm = 0.0739053 (50 iterations in 3.577s)\n",
      "[t-SNE] Iteration 100: error = 61.4530182, gradient norm = 0.0240218 (50 iterations in 2.609s)\n",
      "[t-SNE] Iteration 150: error = 58.5888443, gradient norm = 0.0172877 (50 iterations in 2.546s)\n",
      "[t-SNE] Iteration 200: error = 57.2630386, gradient norm = 0.0141383 (50 iterations in 2.500s)\n",
      "[t-SNE] Iteration 250: error = 56.4693680, gradient norm = 0.0093172 (50 iterations in 2.511s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.469368\n",
      "[t-SNE] Iteration 300: error = 1.3238827, gradient norm = 0.0011953 (50 iterations in 2.683s)\n",
      "[t-SNE] Iteration 350: error = 1.0149492, gradient norm = 0.0004374 (50 iterations in 2.718s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 400: error = 0.8910016, gradient norm = 0.0002654 (50 iterations in 2.685s)\n",
      "[t-SNE] Iteration 450: error = 0.8287245, gradient norm = 0.0001989 (50 iterations in 2.685s)\n",
      "[t-SNE] Iteration 500: error = 0.7936785, gradient norm = 0.0001637 (50 iterations in 2.752s)\n",
      "[t-SNE] Iteration 550: error = 0.7729075, gradient norm = 0.0001400 (50 iterations in 2.898s)\n",
      "[t-SNE] Iteration 600: error = 0.7589135, gradient norm = 0.0001307 (50 iterations in 2.880s)\n",
      "[t-SNE] Iteration 650: error = 0.7492852, gradient norm = 0.0001193 (50 iterations in 2.787s)\n",
      "[t-SNE] Iteration 700: error = 0.7414574, gradient norm = 0.0001111 (50 iterations in 2.697s)\n",
      "[t-SNE] Iteration 750: error = 0.7350124, gradient norm = 0.0000991 (50 iterations in 2.670s)\n",
      "[t-SNE] Iteration 800: error = 0.7296447, gradient norm = 0.0001058 (50 iterations in 2.683s)\n",
      "[t-SNE] Iteration 850: error = 0.7252146, gradient norm = 0.0000970 (50 iterations in 2.657s)\n",
      "[t-SNE] Iteration 900: error = 0.7212129, gradient norm = 0.0000889 (50 iterations in 2.635s)\n",
      "[t-SNE] Iteration 950: error = 0.7172490, gradient norm = 0.0000937 (50 iterations in 2.643s)\n",
      "[t-SNE] Iteration 1000: error = 0.7141298, gradient norm = 0.0000847 (50 iterations in 2.636s)\n",
      "[t-SNE] Error after 1000 iterations: 0.714130\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.133s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 3.674194\n",
      "[t-SNE] Computed conditional probabilities in 0.097s\n",
      "[t-SNE] Iteration 50: error = 74.8513336, gradient norm = 0.0636072 (50 iterations in 3.797s)\n",
      "[t-SNE] Iteration 100: error = 61.4912605, gradient norm = 0.0268116 (50 iterations in 2.618s)\n",
      "[t-SNE] Iteration 150: error = 58.5349541, gradient norm = 0.0185339 (50 iterations in 2.525s)\n",
      "[t-SNE] Iteration 200: error = 57.1752319, gradient norm = 0.0121517 (50 iterations in 2.516s)\n",
      "[t-SNE] Iteration 250: error = 56.3687592, gradient norm = 0.0131863 (50 iterations in 2.531s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.368759\n",
      "[t-SNE] Iteration 300: error = 1.3303076, gradient norm = 0.0011242 (50 iterations in 2.575s)\n",
      "[t-SNE] Iteration 350: error = 1.0207204, gradient norm = 0.0004346 (50 iterations in 2.625s)\n",
      "[t-SNE] Iteration 400: error = 0.8965375, gradient norm = 0.0002655 (50 iterations in 2.608s)\n",
      "[t-SNE] Iteration 450: error = 0.8338034, gradient norm = 0.0001938 (50 iterations in 2.605s)\n",
      "[t-SNE] Iteration 500: error = 0.7981014, gradient norm = 0.0001575 (50 iterations in 2.592s)\n",
      "[t-SNE] Iteration 550: error = 0.7761500, gradient norm = 0.0001337 (50 iterations in 2.596s)\n",
      "[t-SNE] Iteration 600: error = 0.7611005, gradient norm = 0.0001222 (50 iterations in 2.590s)\n",
      "[t-SNE] Iteration 650: error = 0.7506666, gradient norm = 0.0001131 (50 iterations in 2.597s)\n",
      "[t-SNE] Iteration 700: error = 0.7429639, gradient norm = 0.0001165 (50 iterations in 2.585s)\n",
      "[t-SNE] Iteration 750: error = 0.7380795, gradient norm = 0.0001117 (50 iterations in 2.579s)\n",
      "[t-SNE] Iteration 800: error = 0.7341251, gradient norm = 0.0001032 (50 iterations in 2.580s)\n",
      "[t-SNE] Iteration 850: error = 0.7303405, gradient norm = 0.0001014 (50 iterations in 2.554s)\n",
      "[t-SNE] Iteration 900: error = 0.7267460, gradient norm = 0.0000887 (50 iterations in 2.547s)\n",
      "[t-SNE] Iteration 950: error = 0.7228739, gradient norm = 0.0000872 (50 iterations in 2.551s)\n",
      "[t-SNE] Iteration 1000: error = 0.7192305, gradient norm = 0.0000900 (50 iterations in 2.559s)\n",
      "[t-SNE] Error after 1000 iterations: 0.719231\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.138s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 3.774430\n",
      "[t-SNE] Computed conditional probabilities in 0.097s\n",
      "[t-SNE] Iteration 50: error = 74.7333603, gradient norm = 0.0647425 (50 iterations in 3.545s)\n",
      "[t-SNE] Iteration 100: error = 61.3454895, gradient norm = 0.0251870 (50 iterations in 2.628s)\n",
      "[t-SNE] Iteration 150: error = 58.4601593, gradient norm = 0.0179870 (50 iterations in 2.534s)\n",
      "[t-SNE] Iteration 200: error = 57.1446648, gradient norm = 0.0154116 (50 iterations in 2.515s)\n",
      "[t-SNE] Iteration 250: error = 56.3595924, gradient norm = 0.0128646 (50 iterations in 2.511s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.359592\n",
      "[t-SNE] Iteration 300: error = 1.3346999, gradient norm = 0.0011711 (50 iterations in 2.676s)\n",
      "[t-SNE] Iteration 350: error = 1.0264351, gradient norm = 0.0004406 (50 iterations in 2.772s)\n",
      "[t-SNE] Iteration 400: error = 0.9032124, gradient norm = 0.0002661 (50 iterations in 2.749s)\n",
      "[t-SNE] Iteration 450: error = 0.8411953, gradient norm = 0.0001925 (50 iterations in 2.657s)\n",
      "[t-SNE] Iteration 500: error = 0.8063836, gradient norm = 0.0001569 (50 iterations in 2.677s)\n",
      "[t-SNE] Iteration 550: error = 0.7845520, gradient norm = 0.0001328 (50 iterations in 2.646s)\n",
      "[t-SNE] Iteration 600: error = 0.7694120, gradient norm = 0.0001182 (50 iterations in 2.691s)\n",
      "[t-SNE] Iteration 650: error = 0.7582040, gradient norm = 0.0001058 (50 iterations in 2.708s)\n",
      "[t-SNE] Iteration 700: error = 0.7491233, gradient norm = 0.0001062 (50 iterations in 2.704s)\n",
      "[t-SNE] Iteration 750: error = 0.7427628, gradient norm = 0.0001012 (50 iterations in 2.696s)\n",
      "[t-SNE] Iteration 800: error = 0.7373500, gradient norm = 0.0000986 (50 iterations in 2.716s)\n",
      "[t-SNE] Iteration 850: error = 0.7333172, gradient norm = 0.0000824 (50 iterations in 2.654s)\n",
      "[t-SNE] Iteration 900: error = 0.7293757, gradient norm = 0.0000893 (50 iterations in 2.664s)\n",
      "[t-SNE] Iteration 950: error = 0.7264053, gradient norm = 0.0000785 (50 iterations in 2.681s)\n",
      "[t-SNE] Iteration 1000: error = 0.7234413, gradient norm = 0.0000782 (50 iterations in 2.690s)\n",
      "[t-SNE] Error after 1000 iterations: 0.723441\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.148s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 3.876100\n",
      "[t-SNE] Computed conditional probabilities in 0.101s\n",
      "[t-SNE] Iteration 50: error = 74.6838913, gradient norm = 0.0789443 (50 iterations in 4.040s)\n",
      "[t-SNE] Iteration 100: error = 61.4736671, gradient norm = 0.0270795 (50 iterations in 2.719s)\n",
      "[t-SNE] Iteration 150: error = 58.5375443, gradient norm = 0.0146971 (50 iterations in 2.801s)\n",
      "[t-SNE] Iteration 200: error = 57.2061806, gradient norm = 0.0168028 (50 iterations in 2.713s)\n",
      "[t-SNE] Iteration 250: error = 56.4181023, gradient norm = 0.0119606 (50 iterations in 2.549s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.418102\n",
      "[t-SNE] Iteration 300: error = 1.3396446, gradient norm = 0.0011517 (50 iterations in 2.690s)\n",
      "[t-SNE] Iteration 350: error = 1.0307813, gradient norm = 0.0004454 (50 iterations in 2.750s)\n",
      "[t-SNE] Iteration 400: error = 0.9070217, gradient norm = 0.0002676 (50 iterations in 2.759s)\n",
      "[t-SNE] Iteration 450: error = 0.8440374, gradient norm = 0.0001954 (50 iterations in 2.721s)\n",
      "[t-SNE] Iteration 500: error = 0.8080345, gradient norm = 0.0001634 (50 iterations in 2.644s)\n",
      "[t-SNE] Iteration 550: error = 0.7859034, gradient norm = 0.0001442 (50 iterations in 2.664s)\n",
      "[t-SNE] Iteration 600: error = 0.7718100, gradient norm = 0.0001221 (50 iterations in 2.640s)\n",
      "[t-SNE] Iteration 650: error = 0.7607943, gradient norm = 0.0001182 (50 iterations in 2.633s)\n",
      "[t-SNE] Iteration 700: error = 0.7523847, gradient norm = 0.0001063 (50 iterations in 2.641s)\n",
      "[t-SNE] Iteration 750: error = 0.7459269, gradient norm = 0.0001051 (50 iterations in 2.663s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 800: error = 0.7404217, gradient norm = 0.0000955 (50 iterations in 2.641s)\n",
      "[t-SNE] Iteration 850: error = 0.7355732, gradient norm = 0.0000921 (50 iterations in 2.638s)\n",
      "[t-SNE] Iteration 900: error = 0.7310946, gradient norm = 0.0000867 (50 iterations in 2.622s)\n",
      "[t-SNE] Iteration 950: error = 0.7270569, gradient norm = 0.0000871 (50 iterations in 2.630s)\n",
      "[t-SNE] Iteration 1000: error = 0.7238068, gradient norm = 0.0000856 (50 iterations in 2.626s)\n",
      "[t-SNE] Error after 1000 iterations: 0.723807\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.154s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.038185\n",
      "[t-SNE] Computed conditional probabilities in 0.098s\n",
      "[t-SNE] Iteration 50: error = 74.9314194, gradient norm = 0.0808746 (50 iterations in 3.618s)\n",
      "[t-SNE] Iteration 100: error = 61.5119171, gradient norm = 0.0276354 (50 iterations in 2.634s)\n",
      "[t-SNE] Iteration 150: error = 58.5936584, gradient norm = 0.0205527 (50 iterations in 2.536s)\n",
      "[t-SNE] Iteration 200: error = 57.2316093, gradient norm = 0.0122605 (50 iterations in 2.513s)\n",
      "[t-SNE] Iteration 250: error = 56.4360390, gradient norm = 0.0133339 (50 iterations in 2.505s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.436039\n",
      "[t-SNE] Iteration 300: error = 1.3397932, gradient norm = 0.0011229 (50 iterations in 2.649s)\n",
      "[t-SNE] Iteration 350: error = 1.0341522, gradient norm = 0.0004330 (50 iterations in 2.654s)\n",
      "[t-SNE] Iteration 400: error = 0.9102470, gradient norm = 0.0002651 (50 iterations in 2.688s)\n",
      "[t-SNE] Iteration 450: error = 0.8479699, gradient norm = 0.0002005 (50 iterations in 2.697s)\n",
      "[t-SNE] Iteration 500: error = 0.8125880, gradient norm = 0.0001634 (50 iterations in 2.706s)\n",
      "[t-SNE] Iteration 550: error = 0.7903294, gradient norm = 0.0001404 (50 iterations in 2.691s)\n",
      "[t-SNE] Iteration 600: error = 0.7745641, gradient norm = 0.0001229 (50 iterations in 2.690s)\n",
      "[t-SNE] Iteration 650: error = 0.7628741, gradient norm = 0.0001139 (50 iterations in 2.704s)\n",
      "[t-SNE] Iteration 700: error = 0.7544580, gradient norm = 0.0001061 (50 iterations in 2.715s)\n",
      "[t-SNE] Iteration 750: error = 0.7478766, gradient norm = 0.0000975 (50 iterations in 2.684s)\n",
      "[t-SNE] Iteration 800: error = 0.7425432, gradient norm = 0.0000919 (50 iterations in 2.695s)\n",
      "[t-SNE] Iteration 850: error = 0.7374660, gradient norm = 0.0000904 (50 iterations in 2.706s)\n",
      "[t-SNE] Iteration 900: error = 0.7330613, gradient norm = 0.0000966 (50 iterations in 2.692s)\n",
      "[t-SNE] Iteration 950: error = 0.7296314, gradient norm = 0.0000863 (50 iterations in 2.707s)\n",
      "[t-SNE] Iteration 1000: error = 0.7267116, gradient norm = 0.0000850 (50 iterations in 2.694s)\n",
      "[t-SNE] Error after 1000 iterations: 0.726712\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.159s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.167057\n",
      "[t-SNE] Computed conditional probabilities in 0.100s\n",
      "[t-SNE] Iteration 50: error = 74.7705460, gradient norm = 0.0743909 (50 iterations in 3.713s)\n",
      "[t-SNE] Iteration 100: error = 61.5092888, gradient norm = 0.0275035 (50 iterations in 2.685s)\n",
      "[t-SNE] Iteration 150: error = 58.6320839, gradient norm = 0.0221375 (50 iterations in 2.539s)\n",
      "[t-SNE] Iteration 200: error = 57.2843742, gradient norm = 0.0166473 (50 iterations in 2.509s)\n",
      "[t-SNE] Iteration 250: error = 56.4753075, gradient norm = 0.0109549 (50 iterations in 2.518s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.475307\n",
      "[t-SNE] Iteration 300: error = 1.3653433, gradient norm = 0.0011821 (50 iterations in 2.616s)\n",
      "[t-SNE] Iteration 350: error = 1.0519015, gradient norm = 0.0004544 (50 iterations in 2.689s)\n",
      "[t-SNE] Iteration 400: error = 0.9260660, gradient norm = 0.0002859 (50 iterations in 2.734s)\n",
      "[t-SNE] Iteration 450: error = 0.8623942, gradient norm = 0.0002042 (50 iterations in 2.705s)\n",
      "[t-SNE] Iteration 500: error = 0.8247166, gradient norm = 0.0001699 (50 iterations in 2.677s)\n",
      "[t-SNE] Iteration 550: error = 0.8017877, gradient norm = 0.0001477 (50 iterations in 2.689s)\n",
      "[t-SNE] Iteration 600: error = 0.7877041, gradient norm = 0.0001343 (50 iterations in 2.717s)\n",
      "[t-SNE] Iteration 650: error = 0.7779217, gradient norm = 0.0001234 (50 iterations in 2.650s)\n",
      "[t-SNE] Iteration 700: error = 0.7701280, gradient norm = 0.0001120 (50 iterations in 2.644s)\n",
      "[t-SNE] Iteration 750: error = 0.7628248, gradient norm = 0.0001016 (50 iterations in 2.672s)\n",
      "[t-SNE] Iteration 800: error = 0.7565848, gradient norm = 0.0001001 (50 iterations in 2.648s)\n",
      "[t-SNE] Iteration 850: error = 0.7515957, gradient norm = 0.0000897 (50 iterations in 2.646s)\n",
      "[t-SNE] Iteration 900: error = 0.7471008, gradient norm = 0.0000923 (50 iterations in 2.669s)\n",
      "[t-SNE] Iteration 950: error = 0.7433695, gradient norm = 0.0000850 (50 iterations in 2.673s)\n",
      "[t-SNE] Iteration 1000: error = 0.7397620, gradient norm = 0.0000908 (50 iterations in 2.636s)\n",
      "[t-SNE] Error after 1000 iterations: 0.739762\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.175s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.244821\n",
      "[t-SNE] Computed conditional probabilities in 0.101s\n",
      "[t-SNE] Iteration 50: error = 75.2817688, gradient norm = 0.0717187 (50 iterations in 3.912s)\n",
      "[t-SNE] Iteration 100: error = 61.7126427, gradient norm = 0.0259474 (50 iterations in 2.614s)\n",
      "[t-SNE] Iteration 150: error = 58.7336464, gradient norm = 0.0199096 (50 iterations in 2.531s)\n",
      "[t-SNE] Iteration 200: error = 57.3794212, gradient norm = 0.0194799 (50 iterations in 2.488s)\n",
      "[t-SNE] Iteration 250: error = 56.5773010, gradient norm = 0.0143150 (50 iterations in 2.520s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.577301\n",
      "[t-SNE] Iteration 300: error = 1.3675194, gradient norm = 0.0011543 (50 iterations in 2.592s)\n",
      "[t-SNE] Iteration 350: error = 1.0535394, gradient norm = 0.0004442 (50 iterations in 2.685s)\n",
      "[t-SNE] Iteration 400: error = 0.9268677, gradient norm = 0.0002687 (50 iterations in 2.674s)\n",
      "[t-SNE] Iteration 450: error = 0.8636977, gradient norm = 0.0002013 (50 iterations in 2.681s)\n",
      "[t-SNE] Iteration 500: error = 0.8270257, gradient norm = 0.0001659 (50 iterations in 2.680s)\n",
      "[t-SNE] Iteration 550: error = 0.8040913, gradient norm = 0.0001394 (50 iterations in 2.648s)\n",
      "[t-SNE] Iteration 600: error = 0.7886822, gradient norm = 0.0001335 (50 iterations in 2.609s)\n",
      "[t-SNE] Iteration 650: error = 0.7784486, gradient norm = 0.0001232 (50 iterations in 2.618s)\n",
      "[t-SNE] Iteration 700: error = 0.7709844, gradient norm = 0.0001192 (50 iterations in 2.607s)\n",
      "[t-SNE] Iteration 750: error = 0.7652898, gradient norm = 0.0001141 (50 iterations in 2.626s)\n",
      "[t-SNE] Iteration 800: error = 0.7606503, gradient norm = 0.0001007 (50 iterations in 2.642s)\n",
      "[t-SNE] Iteration 850: error = 0.7559062, gradient norm = 0.0001012 (50 iterations in 2.644s)\n",
      "[t-SNE] Iteration 900: error = 0.7516370, gradient norm = 0.0001004 (50 iterations in 2.645s)\n",
      "[t-SNE] Iteration 950: error = 0.7481965, gradient norm = 0.0000893 (50 iterations in 2.646s)\n",
      "[t-SNE] Iteration 1000: error = 0.7449170, gradient norm = 0.0000903 (50 iterations in 2.653s)\n",
      "[t-SNE] Error after 1000 iterations: 0.744917\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.175s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.329996\n",
      "[t-SNE] Computed conditional probabilities in 0.106s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 50: error = 75.2698059, gradient norm = 0.0851933 (50 iterations in 4.076s)\n",
      "[t-SNE] Iteration 100: error = 61.7148361, gradient norm = 0.0288568 (50 iterations in 2.648s)\n",
      "[t-SNE] Iteration 150: error = 58.7981758, gradient norm = 0.0206261 (50 iterations in 2.583s)\n",
      "[t-SNE] Iteration 200: error = 57.4445648, gradient norm = 0.0160942 (50 iterations in 2.605s)\n",
      "[t-SNE] Iteration 250: error = 56.6599388, gradient norm = 0.0106631 (50 iterations in 2.534s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.659939\n",
      "[t-SNE] Iteration 300: error = 1.3704100, gradient norm = 0.0011384 (50 iterations in 2.687s)\n",
      "[t-SNE] Iteration 350: error = 1.0636597, gradient norm = 0.0004410 (50 iterations in 2.748s)\n",
      "[t-SNE] Iteration 400: error = 0.9391515, gradient norm = 0.0002887 (50 iterations in 2.728s)\n",
      "[t-SNE] Iteration 450: error = 0.8768442, gradient norm = 0.0001999 (50 iterations in 2.744s)\n",
      "[t-SNE] Iteration 500: error = 0.8398266, gradient norm = 0.0001692 (50 iterations in 2.720s)\n",
      "[t-SNE] Iteration 550: error = 0.8168379, gradient norm = 0.0001461 (50 iterations in 2.754s)\n",
      "[t-SNE] Iteration 600: error = 0.8011788, gradient norm = 0.0001349 (50 iterations in 2.735s)\n",
      "[t-SNE] Iteration 650: error = 0.7901764, gradient norm = 0.0001245 (50 iterations in 2.721s)\n",
      "[t-SNE] Iteration 700: error = 0.7819766, gradient norm = 0.0001142 (50 iterations in 2.759s)\n",
      "[t-SNE] Iteration 750: error = 0.7748641, gradient norm = 0.0001050 (50 iterations in 2.720s)\n",
      "[t-SNE] Iteration 800: error = 0.7688518, gradient norm = 0.0001017 (50 iterations in 2.689s)\n",
      "[t-SNE] Iteration 850: error = 0.7639253, gradient norm = 0.0000971 (50 iterations in 2.717s)\n",
      "[t-SNE] Iteration 900: error = 0.7599210, gradient norm = 0.0001079 (50 iterations in 2.723s)\n",
      "[t-SNE] Iteration 950: error = 0.7569258, gradient norm = 0.0000889 (50 iterations in 2.719s)\n",
      "[t-SNE] Iteration 1000: error = 0.7537618, gradient norm = 0.0000857 (50 iterations in 2.726s)\n",
      "[t-SNE] Error after 1000 iterations: 0.753762\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.179s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.392779\n",
      "[t-SNE] Computed conditional probabilities in 0.109s\n",
      "[t-SNE] Iteration 50: error = 74.9228745, gradient norm = 0.0802434 (50 iterations in 3.730s)\n",
      "[t-SNE] Iteration 100: error = 61.7742653, gradient norm = 0.0248992 (50 iterations in 2.710s)\n",
      "[t-SNE] Iteration 150: error = 58.9544296, gradient norm = 0.0189769 (50 iterations in 2.612s)\n",
      "[t-SNE] Iteration 200: error = 57.6413040, gradient norm = 0.0217559 (50 iterations in 2.558s)\n",
      "[t-SNE] Iteration 250: error = 56.8617134, gradient norm = 0.0160769 (50 iterations in 2.572s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.861713\n",
      "[t-SNE] Iteration 300: error = 1.3850377, gradient norm = 0.0011456 (50 iterations in 2.668s)\n",
      "[t-SNE] Iteration 350: error = 1.0747194, gradient norm = 0.0004566 (50 iterations in 2.718s)\n",
      "[t-SNE] Iteration 400: error = 0.9484820, gradient norm = 0.0002816 (50 iterations in 2.698s)\n",
      "[t-SNE] Iteration 450: error = 0.8844953, gradient norm = 0.0002087 (50 iterations in 2.709s)\n",
      "[t-SNE] Iteration 500: error = 0.8468401, gradient norm = 0.0001746 (50 iterations in 2.694s)\n",
      "[t-SNE] Iteration 550: error = 0.8226395, gradient norm = 0.0001498 (50 iterations in 2.702s)\n",
      "[t-SNE] Iteration 600: error = 0.8071061, gradient norm = 0.0001280 (50 iterations in 2.689s)\n",
      "[t-SNE] Iteration 650: error = 0.7956035, gradient norm = 0.0001177 (50 iterations in 2.703s)\n",
      "[t-SNE] Iteration 700: error = 0.7864396, gradient norm = 0.0001105 (50 iterations in 2.703s)\n",
      "[t-SNE] Iteration 750: error = 0.7787877, gradient norm = 0.0001127 (50 iterations in 2.715s)\n",
      "[t-SNE] Iteration 800: error = 0.7722832, gradient norm = 0.0001032 (50 iterations in 2.687s)\n",
      "[t-SNE] Iteration 850: error = 0.7668278, gradient norm = 0.0001008 (50 iterations in 2.692s)\n",
      "[t-SNE] Iteration 900: error = 0.7625666, gradient norm = 0.0000918 (50 iterations in 2.706s)\n",
      "[t-SNE] Iteration 950: error = 0.7584758, gradient norm = 0.0000944 (50 iterations in 2.701s)\n",
      "[t-SNE] Iteration 1000: error = 0.7548277, gradient norm = 0.0000888 (50 iterations in 2.667s)\n",
      "[t-SNE] Error after 1000 iterations: 0.754828\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.184s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.637252\n",
      "[t-SNE] Computed conditional probabilities in 0.105s\n",
      "[t-SNE] Iteration 50: error = 75.0371094, gradient norm = 0.0774334 (50 iterations in 4.130s)\n",
      "[t-SNE] Iteration 100: error = 61.6343155, gradient norm = 0.0263345 (50 iterations in 2.638s)\n",
      "[t-SNE] Iteration 150: error = 58.7880249, gradient norm = 0.0226763 (50 iterations in 2.530s)\n",
      "[t-SNE] Iteration 200: error = 57.4968033, gradient norm = 0.0158657 (50 iterations in 2.517s)\n",
      "[t-SNE] Iteration 250: error = 56.7308578, gradient norm = 0.0128535 (50 iterations in 2.489s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.730858\n",
      "[t-SNE] Iteration 300: error = 1.3806721, gradient norm = 0.0011261 (50 iterations in 2.649s)\n",
      "[t-SNE] Iteration 350: error = 1.0704219, gradient norm = 0.0004411 (50 iterations in 2.717s)\n",
      "[t-SNE] Iteration 400: error = 0.9446598, gradient norm = 0.0002761 (50 iterations in 2.716s)\n",
      "[t-SNE] Iteration 450: error = 0.8818511, gradient norm = 0.0002071 (50 iterations in 2.710s)\n",
      "[t-SNE] Iteration 500: error = 0.8454884, gradient norm = 0.0001706 (50 iterations in 2.706s)\n",
      "[t-SNE] Iteration 550: error = 0.8234544, gradient norm = 0.0001549 (50 iterations in 2.754s)\n",
      "[t-SNE] Iteration 600: error = 0.8091432, gradient norm = 0.0001368 (50 iterations in 2.812s)\n",
      "[t-SNE] Iteration 650: error = 0.7979232, gradient norm = 0.0001214 (50 iterations in 2.791s)\n",
      "[t-SNE] Iteration 700: error = 0.7886153, gradient norm = 0.0001233 (50 iterations in 2.817s)\n",
      "[t-SNE] Iteration 750: error = 0.7812885, gradient norm = 0.0001127 (50 iterations in 2.715s)\n",
      "[t-SNE] Iteration 800: error = 0.7752184, gradient norm = 0.0001118 (50 iterations in 2.713s)\n",
      "[t-SNE] Iteration 850: error = 0.7703987, gradient norm = 0.0001010 (50 iterations in 2.712s)\n",
      "[t-SNE] Iteration 900: error = 0.7665504, gradient norm = 0.0000928 (50 iterations in 2.695s)\n",
      "[t-SNE] Iteration 950: error = 0.7626655, gradient norm = 0.0000932 (50 iterations in 2.692s)\n",
      "[t-SNE] Iteration 1000: error = 0.7585391, gradient norm = 0.0001000 (50 iterations in 2.715s)\n",
      "[t-SNE] Error after 1000 iterations: 0.758539\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.201s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.688410\n",
      "[t-SNE] Computed conditional probabilities in 0.111s\n",
      "[t-SNE] Iteration 50: error = 75.2106781, gradient norm = 0.0795468 (50 iterations in 4.400s)\n",
      "[t-SNE] Iteration 100: error = 61.7981834, gradient norm = 0.0369058 (50 iterations in 2.727s)\n",
      "[t-SNE] Iteration 150: error = 58.9850845, gradient norm = 0.0190377 (50 iterations in 2.631s)\n",
      "[t-SNE] Iteration 200: error = 57.7010689, gradient norm = 0.0251842 (50 iterations in 2.588s)\n",
      "[t-SNE] Iteration 250: error = 56.9271507, gradient norm = 0.0157066 (50 iterations in 2.541s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.927151\n",
      "[t-SNE] Iteration 300: error = 1.3824865, gradient norm = 0.0011384 (50 iterations in 2.719s)\n",
      "[t-SNE] Iteration 350: error = 1.0745763, gradient norm = 0.0004482 (50 iterations in 2.775s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 400: error = 0.9490552, gradient norm = 0.0002788 (50 iterations in 2.777s)\n",
      "[t-SNE] Iteration 450: error = 0.8856910, gradient norm = 0.0002147 (50 iterations in 2.783s)\n",
      "[t-SNE] Iteration 500: error = 0.8500558, gradient norm = 0.0001681 (50 iterations in 2.748s)\n",
      "[t-SNE] Iteration 550: error = 0.8270851, gradient norm = 0.0001475 (50 iterations in 2.750s)\n",
      "[t-SNE] Iteration 600: error = 0.8106479, gradient norm = 0.0001264 (50 iterations in 2.746s)\n",
      "[t-SNE] Iteration 650: error = 0.7978984, gradient norm = 0.0001214 (50 iterations in 2.779s)\n",
      "[t-SNE] Iteration 700: error = 0.7876773, gradient norm = 0.0001194 (50 iterations in 2.752s)\n",
      "[t-SNE] Iteration 750: error = 0.7801803, gradient norm = 0.0001159 (50 iterations in 2.773s)\n",
      "[t-SNE] Iteration 800: error = 0.7748540, gradient norm = 0.0001116 (50 iterations in 2.747s)\n",
      "[t-SNE] Iteration 850: error = 0.7702771, gradient norm = 0.0001007 (50 iterations in 2.740s)\n",
      "[t-SNE] Iteration 900: error = 0.7657656, gradient norm = 0.0000987 (50 iterations in 2.760s)\n",
      "[t-SNE] Iteration 950: error = 0.7616802, gradient norm = 0.0000924 (50 iterations in 2.756s)\n",
      "[t-SNE] Iteration 1000: error = 0.7578372, gradient norm = 0.0000823 (50 iterations in 2.742s)\n",
      "[t-SNE] Error after 1000 iterations: 0.757837\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.198s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.779943\n",
      "[t-SNE] Computed conditional probabilities in 0.111s\n",
      "[t-SNE] Iteration 50: error = 75.2956009, gradient norm = 0.0914707 (50 iterations in 4.464s)\n",
      "[t-SNE] Iteration 100: error = 62.1022224, gradient norm = 0.0297928 (50 iterations in 2.723s)\n",
      "[t-SNE] Iteration 150: error = 59.1085014, gradient norm = 0.0173347 (50 iterations in 2.608s)\n",
      "[t-SNE] Iteration 200: error = 57.7802963, gradient norm = 0.0197858 (50 iterations in 2.593s)\n",
      "[t-SNE] Iteration 250: error = 56.9895897, gradient norm = 0.0163055 (50 iterations in 2.567s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 56.989590\n",
      "[t-SNE] Iteration 300: error = 1.3805681, gradient norm = 0.0011299 (50 iterations in 2.673s)\n",
      "[t-SNE] Iteration 350: error = 1.0752188, gradient norm = 0.0004494 (50 iterations in 2.771s)\n",
      "[t-SNE] Iteration 400: error = 0.9494264, gradient norm = 0.0002734 (50 iterations in 2.778s)\n",
      "[t-SNE] Iteration 450: error = 0.8850209, gradient norm = 0.0002119 (50 iterations in 2.772s)\n",
      "[t-SNE] Iteration 500: error = 0.8470810, gradient norm = 0.0001702 (50 iterations in 2.763s)\n",
      "[t-SNE] Iteration 550: error = 0.8234652, gradient norm = 0.0001489 (50 iterations in 2.793s)\n",
      "[t-SNE] Iteration 600: error = 0.8066732, gradient norm = 0.0001312 (50 iterations in 2.760s)\n",
      "[t-SNE] Iteration 650: error = 0.7945307, gradient norm = 0.0001219 (50 iterations in 2.763s)\n",
      "[t-SNE] Iteration 700: error = 0.7851349, gradient norm = 0.0001198 (50 iterations in 2.728s)\n",
      "[t-SNE] Iteration 750: error = 0.7780164, gradient norm = 0.0001152 (50 iterations in 2.821s)\n",
      "[t-SNE] Iteration 800: error = 0.7719075, gradient norm = 0.0001095 (50 iterations in 2.762s)\n",
      "[t-SNE] Iteration 850: error = 0.7668917, gradient norm = 0.0001001 (50 iterations in 2.735s)\n",
      "[t-SNE] Iteration 900: error = 0.7623503, gradient norm = 0.0000977 (50 iterations in 2.802s)\n",
      "[t-SNE] Iteration 950: error = 0.7578798, gradient norm = 0.0000927 (50 iterations in 2.703s)\n",
      "[t-SNE] Iteration 1000: error = 0.7540777, gradient norm = 0.0000910 (50 iterations in 2.736s)\n",
      "[t-SNE] Error after 1000 iterations: 0.754078\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.206s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.720362\n",
      "[t-SNE] Computed conditional probabilities in 0.117s\n",
      "[t-SNE] Iteration 50: error = 75.6259079, gradient norm = 0.0855208 (50 iterations in 3.875s)\n",
      "[t-SNE] Iteration 100: error = 62.1140747, gradient norm = 0.0281091 (50 iterations in 2.742s)\n",
      "[t-SNE] Iteration 150: error = 59.3082199, gradient norm = 0.0243290 (50 iterations in 2.650s)\n",
      "[t-SNE] Iteration 200: error = 58.0325317, gradient norm = 0.0129187 (50 iterations in 2.564s)\n",
      "[t-SNE] Iteration 250: error = 57.2961998, gradient norm = 0.0186453 (50 iterations in 2.555s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.296200\n",
      "[t-SNE] Iteration 300: error = 1.3987085, gradient norm = 0.0011107 (50 iterations in 2.753s)\n",
      "[t-SNE] Iteration 350: error = 1.0896941, gradient norm = 0.0004484 (50 iterations in 2.778s)\n",
      "[t-SNE] Iteration 400: error = 0.9620423, gradient norm = 0.0002918 (50 iterations in 2.836s)\n",
      "[t-SNE] Iteration 450: error = 0.8978350, gradient norm = 0.0002146 (50 iterations in 2.873s)\n",
      "[t-SNE] Iteration 500: error = 0.8599719, gradient norm = 0.0001767 (50 iterations in 2.766s)\n",
      "[t-SNE] Iteration 550: error = 0.8352951, gradient norm = 0.0001527 (50 iterations in 2.779s)\n",
      "[t-SNE] Iteration 600: error = 0.8178828, gradient norm = 0.0001378 (50 iterations in 2.785s)\n",
      "[t-SNE] Iteration 650: error = 0.8054518, gradient norm = 0.0001255 (50 iterations in 2.767s)\n",
      "[t-SNE] Iteration 700: error = 0.7954296, gradient norm = 0.0001170 (50 iterations in 2.850s)\n",
      "[t-SNE] Iteration 750: error = 0.7868993, gradient norm = 0.0001075 (50 iterations in 2.776s)\n",
      "[t-SNE] Iteration 800: error = 0.7796631, gradient norm = 0.0000997 (50 iterations in 2.748s)\n",
      "[t-SNE] Iteration 850: error = 0.7733023, gradient norm = 0.0000997 (50 iterations in 2.769s)\n",
      "[t-SNE] Iteration 900: error = 0.7677401, gradient norm = 0.0000937 (50 iterations in 2.819s)\n",
      "[t-SNE] Iteration 950: error = 0.7632943, gradient norm = 0.0000908 (50 iterations in 2.754s)\n",
      "[t-SNE] Iteration 1000: error = 0.7593817, gradient norm = 0.0000921 (50 iterations in 2.782s)\n",
      "[t-SNE] Error after 1000 iterations: 0.759382\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.221s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.701028\n",
      "[t-SNE] Computed conditional probabilities in 0.120s\n",
      "[t-SNE] Iteration 50: error = 75.8326263, gradient norm = 0.0847342 (50 iterations in 3.911s)\n",
      "[t-SNE] Iteration 100: error = 62.0540428, gradient norm = 0.0285260 (50 iterations in 2.785s)\n",
      "[t-SNE] Iteration 150: error = 59.2863197, gradient norm = 0.0177305 (50 iterations in 2.648s)\n",
      "[t-SNE] Iteration 200: error = 58.0191498, gradient norm = 0.0165593 (50 iterations in 2.583s)\n",
      "[t-SNE] Iteration 250: error = 57.2671089, gradient norm = 0.0111288 (50 iterations in 2.595s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.267109\n",
      "[t-SNE] Iteration 300: error = 1.4079487, gradient norm = 0.0011447 (50 iterations in 2.723s)\n",
      "[t-SNE] Iteration 350: error = 1.0950986, gradient norm = 0.0004564 (50 iterations in 2.782s)\n",
      "[t-SNE] Iteration 400: error = 0.9676332, gradient norm = 0.0002918 (50 iterations in 2.781s)\n",
      "[t-SNE] Iteration 450: error = 0.9029337, gradient norm = 0.0002215 (50 iterations in 2.755s)\n",
      "[t-SNE] Iteration 500: error = 0.8640298, gradient norm = 0.0001786 (50 iterations in 2.792s)\n",
      "[t-SNE] Iteration 550: error = 0.8386680, gradient norm = 0.0001567 (50 iterations in 2.785s)\n",
      "[t-SNE] Iteration 600: error = 0.8211938, gradient norm = 0.0001471 (50 iterations in 2.772s)\n",
      "[t-SNE] Iteration 650: error = 0.8083271, gradient norm = 0.0001293 (50 iterations in 2.794s)\n",
      "[t-SNE] Iteration 700: error = 0.7980500, gradient norm = 0.0001268 (50 iterations in 2.725s)\n",
      "[t-SNE] Iteration 750: error = 0.7899606, gradient norm = 0.0001150 (50 iterations in 2.741s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 800: error = 0.7832481, gradient norm = 0.0001120 (50 iterations in 2.735s)\n",
      "[t-SNE] Iteration 850: error = 0.7776827, gradient norm = 0.0001040 (50 iterations in 2.791s)\n",
      "[t-SNE] Iteration 900: error = 0.7729297, gradient norm = 0.0001077 (50 iterations in 2.780s)\n",
      "[t-SNE] Iteration 950: error = 0.7687666, gradient norm = 0.0000897 (50 iterations in 2.779s)\n",
      "[t-SNE] Iteration 1000: error = 0.7646447, gradient norm = 0.0000970 (50 iterations in 2.749s)\n",
      "[t-SNE] Error after 1000 iterations: 0.764645\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.216s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.960495\n",
      "[t-SNE] Computed conditional probabilities in 0.114s\n",
      "[t-SNE] Iteration 50: error = 76.0609207, gradient norm = 0.0816725 (50 iterations in 4.211s)\n",
      "[t-SNE] Iteration 100: error = 62.0804520, gradient norm = 0.0326053 (50 iterations in 2.802s)\n",
      "[t-SNE] Iteration 150: error = 59.2776451, gradient norm = 0.0250214 (50 iterations in 2.675s)\n",
      "[t-SNE] Iteration 200: error = 58.0150719, gradient norm = 0.0189444 (50 iterations in 2.633s)\n",
      "[t-SNE] Iteration 250: error = 57.2577705, gradient norm = 0.0148106 (50 iterations in 2.634s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.257771\n",
      "[t-SNE] Iteration 300: error = 1.4004362, gradient norm = 0.0011345 (50 iterations in 2.754s)\n",
      "[t-SNE] Iteration 350: error = 1.0903991, gradient norm = 0.0004673 (50 iterations in 2.865s)\n",
      "[t-SNE] Iteration 400: error = 0.9619141, gradient norm = 0.0002917 (50 iterations in 2.810s)\n",
      "[t-SNE] Iteration 450: error = 0.8962364, gradient norm = 0.0002256 (50 iterations in 2.805s)\n",
      "[t-SNE] Iteration 500: error = 0.8585271, gradient norm = 0.0001817 (50 iterations in 2.866s)\n",
      "[t-SNE] Iteration 550: error = 0.8337619, gradient norm = 0.0001568 (50 iterations in 2.818s)\n",
      "[t-SNE] Iteration 600: error = 0.8162814, gradient norm = 0.0001386 (50 iterations in 2.861s)\n",
      "[t-SNE] Iteration 650: error = 0.8033839, gradient norm = 0.0001293 (50 iterations in 2.811s)\n",
      "[t-SNE] Iteration 700: error = 0.7931780, gradient norm = 0.0001179 (50 iterations in 2.835s)\n",
      "[t-SNE] Iteration 750: error = 0.7849193, gradient norm = 0.0001069 (50 iterations in 2.791s)\n",
      "[t-SNE] Iteration 800: error = 0.7774060, gradient norm = 0.0001008 (50 iterations in 2.833s)\n",
      "[t-SNE] Iteration 850: error = 0.7710825, gradient norm = 0.0001016 (50 iterations in 2.832s)\n",
      "[t-SNE] Iteration 900: error = 0.7660385, gradient norm = 0.0000957 (50 iterations in 2.871s)\n",
      "[t-SNE] Iteration 950: error = 0.7613922, gradient norm = 0.0000997 (50 iterations in 2.813s)\n",
      "[t-SNE] Iteration 1000: error = 0.7578932, gradient norm = 0.0000911 (50 iterations in 2.759s)\n",
      "[t-SNE] Error after 1000 iterations: 0.757893\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.222s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.048225\n",
      "[t-SNE] Computed conditional probabilities in 0.114s\n",
      "[t-SNE] Iteration 50: error = 75.8774338, gradient norm = 0.0933366 (50 iterations in 4.127s)\n",
      "[t-SNE] Iteration 100: error = 61.9653397, gradient norm = 0.0331568 (50 iterations in 2.721s)\n",
      "[t-SNE] Iteration 150: error = 59.2018623, gradient norm = 0.0201585 (50 iterations in 2.619s)\n",
      "[t-SNE] Iteration 200: error = 57.9470177, gradient norm = 0.0203714 (50 iterations in 2.631s)\n",
      "[t-SNE] Iteration 250: error = 57.1974068, gradient norm = 0.0133249 (50 iterations in 2.580s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.197407\n",
      "[t-SNE] Iteration 300: error = 1.4025970, gradient norm = 0.0011467 (50 iterations in 2.726s)\n",
      "[t-SNE] Iteration 350: error = 1.0898049, gradient norm = 0.0004538 (50 iterations in 2.761s)\n",
      "[t-SNE] Iteration 400: error = 0.9609356, gradient norm = 0.0002879 (50 iterations in 2.764s)\n",
      "[t-SNE] Iteration 450: error = 0.8952687, gradient norm = 0.0002155 (50 iterations in 2.782s)\n",
      "[t-SNE] Iteration 500: error = 0.8561659, gradient norm = 0.0001761 (50 iterations in 2.820s)\n",
      "[t-SNE] Iteration 550: error = 0.8299154, gradient norm = 0.0001561 (50 iterations in 2.785s)\n",
      "[t-SNE] Iteration 600: error = 0.8114550, gradient norm = 0.0001354 (50 iterations in 2.816s)\n",
      "[t-SNE] Iteration 650: error = 0.7974368, gradient norm = 0.0001237 (50 iterations in 2.756s)\n",
      "[t-SNE] Iteration 700: error = 0.7868100, gradient norm = 0.0001218 (50 iterations in 2.836s)\n",
      "[t-SNE] Iteration 750: error = 0.7792789, gradient norm = 0.0001093 (50 iterations in 2.744s)\n",
      "[t-SNE] Iteration 800: error = 0.7724319, gradient norm = 0.0001016 (50 iterations in 2.824s)\n",
      "[t-SNE] Iteration 850: error = 0.7665948, gradient norm = 0.0000958 (50 iterations in 2.796s)\n",
      "[t-SNE] Iteration 900: error = 0.7618779, gradient norm = 0.0000938 (50 iterations in 2.766s)\n",
      "[t-SNE] Iteration 950: error = 0.7577484, gradient norm = 0.0000905 (50 iterations in 2.786s)\n",
      "[t-SNE] Iteration 1000: error = 0.7537565, gradient norm = 0.0000825 (50 iterations in 2.735s)\n",
      "[t-SNE] Error after 1000 iterations: 0.753757\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.236s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.979463\n",
      "[t-SNE] Computed conditional probabilities in 0.118s\n",
      "[t-SNE] Iteration 50: error = 76.5243683, gradient norm = 0.0884155 (50 iterations in 4.245s)\n",
      "[t-SNE] Iteration 100: error = 62.0119705, gradient norm = 0.0360456 (50 iterations in 2.793s)\n",
      "[t-SNE] Iteration 150: error = 59.2623520, gradient norm = 0.0221822 (50 iterations in 2.614s)\n",
      "[t-SNE] Iteration 200: error = 58.0186043, gradient norm = 0.0206094 (50 iterations in 2.562s)\n",
      "[t-SNE] Iteration 250: error = 57.2883148, gradient norm = 0.0143930 (50 iterations in 2.556s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.288315\n",
      "[t-SNE] Iteration 300: error = 1.4093348, gradient norm = 0.0011501 (50 iterations in 2.697s)\n",
      "[t-SNE] Iteration 350: error = 1.0969526, gradient norm = 0.0004674 (50 iterations in 2.773s)\n",
      "[t-SNE] Iteration 400: error = 0.9660493, gradient norm = 0.0003038 (50 iterations in 2.770s)\n",
      "[t-SNE] Iteration 450: error = 0.8998632, gradient norm = 0.0002263 (50 iterations in 2.770s)\n",
      "[t-SNE] Iteration 500: error = 0.8605103, gradient norm = 0.0001847 (50 iterations in 2.764s)\n",
      "[t-SNE] Iteration 550: error = 0.8339379, gradient norm = 0.0001518 (50 iterations in 2.765s)\n",
      "[t-SNE] Iteration 600: error = 0.8150303, gradient norm = 0.0001347 (50 iterations in 2.759s)\n",
      "[t-SNE] Iteration 650: error = 0.7999460, gradient norm = 0.0001256 (50 iterations in 2.759s)\n",
      "[t-SNE] Iteration 700: error = 0.7886367, gradient norm = 0.0001095 (50 iterations in 2.743s)\n",
      "[t-SNE] Iteration 750: error = 0.7795897, gradient norm = 0.0001053 (50 iterations in 2.744s)\n",
      "[t-SNE] Iteration 800: error = 0.7719948, gradient norm = 0.0000990 (50 iterations in 2.750s)\n",
      "[t-SNE] Iteration 850: error = 0.7655331, gradient norm = 0.0000965 (50 iterations in 2.741s)\n",
      "[t-SNE] Iteration 900: error = 0.7601778, gradient norm = 0.0000911 (50 iterations in 2.739s)\n",
      "[t-SNE] Iteration 950: error = 0.7553040, gradient norm = 0.0000915 (50 iterations in 2.743s)\n",
      "[t-SNE] Iteration 1000: error = 0.7509986, gradient norm = 0.0000914 (50 iterations in 2.733s)\n",
      "[t-SNE] Error after 1000 iterations: 0.750999\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.230s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.014525\n",
      "[t-SNE] Computed conditional probabilities in 0.120s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 50: error = 76.5614624, gradient norm = 0.0784026 (50 iterations in 4.251s)\n",
      "[t-SNE] Iteration 100: error = 62.1524162, gradient norm = 0.0260040 (50 iterations in 2.741s)\n",
      "[t-SNE] Iteration 150: error = 59.3687134, gradient norm = 0.0180726 (50 iterations in 2.561s)\n",
      "[t-SNE] Iteration 200: error = 58.1347504, gradient norm = 0.0196076 (50 iterations in 2.546s)\n",
      "[t-SNE] Iteration 250: error = 57.4094391, gradient norm = 0.0111758 (50 iterations in 2.524s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.409439\n",
      "[t-SNE] Iteration 300: error = 1.4135298, gradient norm = 0.0011105 (50 iterations in 2.739s)\n",
      "[t-SNE] Iteration 350: error = 1.1029763, gradient norm = 0.0004546 (50 iterations in 2.769s)\n",
      "[t-SNE] Iteration 400: error = 0.9729591, gradient norm = 0.0002947 (50 iterations in 2.813s)\n",
      "[t-SNE] Iteration 450: error = 0.9061055, gradient norm = 0.0002217 (50 iterations in 2.789s)\n",
      "[t-SNE] Iteration 500: error = 0.8668475, gradient norm = 0.0001852 (50 iterations in 2.803s)\n",
      "[t-SNE] Iteration 550: error = 0.8418027, gradient norm = 0.0001610 (50 iterations in 2.841s)\n",
      "[t-SNE] Iteration 600: error = 0.8236206, gradient norm = 0.0001419 (50 iterations in 2.806s)\n",
      "[t-SNE] Iteration 650: error = 0.8096445, gradient norm = 0.0001329 (50 iterations in 2.821s)\n",
      "[t-SNE] Iteration 700: error = 0.7984985, gradient norm = 0.0001196 (50 iterations in 2.807s)\n",
      "[t-SNE] Iteration 750: error = 0.7902852, gradient norm = 0.0001112 (50 iterations in 2.764s)\n",
      "[t-SNE] Iteration 800: error = 0.7832763, gradient norm = 0.0001136 (50 iterations in 2.785s)\n",
      "[t-SNE] Iteration 850: error = 0.7773483, gradient norm = 0.0001123 (50 iterations in 2.755s)\n",
      "[t-SNE] Iteration 900: error = 0.7721632, gradient norm = 0.0001049 (50 iterations in 2.792s)\n",
      "[t-SNE] Iteration 950: error = 0.7671741, gradient norm = 0.0000990 (50 iterations in 2.814s)\n",
      "[t-SNE] Iteration 1000: error = 0.7625085, gradient norm = 0.0000961 (50 iterations in 2.831s)\n",
      "[t-SNE] Error after 1000 iterations: 0.762509\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.239s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 4.985353\n",
      "[t-SNE] Computed conditional probabilities in 0.125s\n",
      "[t-SNE] Iteration 50: error = 75.8736801, gradient norm = 0.0906018 (50 iterations in 4.058s)\n",
      "[t-SNE] Iteration 100: error = 62.1924438, gradient norm = 0.0305939 (50 iterations in 2.703s)\n",
      "[t-SNE] Iteration 150: error = 59.5059929, gradient norm = 0.0263314 (50 iterations in 2.603s)\n",
      "[t-SNE] Iteration 200: error = 58.2837601, gradient norm = 0.0208068 (50 iterations in 2.604s)\n",
      "[t-SNE] Iteration 250: error = 57.5620499, gradient norm = 0.0184622 (50 iterations in 2.737s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.562050\n",
      "[t-SNE] Iteration 300: error = 1.4138887, gradient norm = 0.0011438 (50 iterations in 2.888s)\n",
      "[t-SNE] Iteration 350: error = 1.1029410, gradient norm = 0.0004577 (50 iterations in 3.012s)\n",
      "[t-SNE] Iteration 400: error = 0.9730569, gradient norm = 0.0003012 (50 iterations in 3.030s)\n",
      "[t-SNE] Iteration 450: error = 0.9056815, gradient norm = 0.0002245 (50 iterations in 2.973s)\n",
      "[t-SNE] Iteration 500: error = 0.8648846, gradient norm = 0.0001839 (50 iterations in 2.940s)\n",
      "[t-SNE] Iteration 550: error = 0.8380035, gradient norm = 0.0001567 (50 iterations in 3.014s)\n",
      "[t-SNE] Iteration 600: error = 0.8185720, gradient norm = 0.0001398 (50 iterations in 2.999s)\n",
      "[t-SNE] Iteration 650: error = 0.8038965, gradient norm = 0.0001234 (50 iterations in 3.031s)\n",
      "[t-SNE] Iteration 700: error = 0.7918962, gradient norm = 0.0001155 (50 iterations in 2.955s)\n",
      "[t-SNE] Iteration 750: error = 0.7824332, gradient norm = 0.0001036 (50 iterations in 2.969s)\n",
      "[t-SNE] Iteration 800: error = 0.7745966, gradient norm = 0.0001034 (50 iterations in 2.978s)\n",
      "[t-SNE] Iteration 850: error = 0.7681592, gradient norm = 0.0000924 (50 iterations in 2.962s)\n",
      "[t-SNE] Iteration 900: error = 0.7622838, gradient norm = 0.0000891 (50 iterations in 2.985s)\n",
      "[t-SNE] Iteration 950: error = 0.7569252, gradient norm = 0.0000881 (50 iterations in 3.000s)\n",
      "[t-SNE] Iteration 1000: error = 0.7524685, gradient norm = 0.0000842 (50 iterations in 2.984s)\n",
      "[t-SNE] Error after 1000 iterations: 0.752468\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.270s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.047911\n",
      "[t-SNE] Computed conditional probabilities in 0.131s\n",
      "[t-SNE] Iteration 50: error = 76.1637573, gradient norm = 0.0852761 (50 iterations in 4.753s)\n",
      "[t-SNE] Iteration 100: error = 62.3073273, gradient norm = 0.0303390 (50 iterations in 2.940s)\n",
      "[t-SNE] Iteration 150: error = 59.6020279, gradient norm = 0.0200071 (50 iterations in 2.838s)\n",
      "[t-SNE] Iteration 200: error = 58.3743286, gradient norm = 0.0136936 (50 iterations in 2.700s)\n",
      "[t-SNE] Iteration 250: error = 57.6583900, gradient norm = 0.0117371 (50 iterations in 2.683s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.658390\n",
      "[t-SNE] Iteration 300: error = 1.4265167, gradient norm = 0.0011694 (50 iterations in 2.866s)\n",
      "[t-SNE] Iteration 350: error = 1.1114055, gradient norm = 0.0004611 (50 iterations in 3.001s)\n",
      "[t-SNE] Iteration 400: error = 0.9795372, gradient norm = 0.0003048 (50 iterations in 2.987s)\n",
      "[t-SNE] Iteration 450: error = 0.9108448, gradient norm = 0.0002352 (50 iterations in 3.010s)\n",
      "[t-SNE] Iteration 500: error = 0.8693569, gradient norm = 0.0001871 (50 iterations in 2.995s)\n",
      "[t-SNE] Iteration 550: error = 0.8420413, gradient norm = 0.0001611 (50 iterations in 2.965s)\n",
      "[t-SNE] Iteration 600: error = 0.8235086, gradient norm = 0.0001456 (50 iterations in 2.980s)\n",
      "[t-SNE] Iteration 650: error = 0.8111110, gradient norm = 0.0001339 (50 iterations in 2.970s)\n",
      "[t-SNE] Iteration 700: error = 0.8018944, gradient norm = 0.0001234 (50 iterations in 2.937s)\n",
      "[t-SNE] Iteration 750: error = 0.7934715, gradient norm = 0.0001153 (50 iterations in 2.928s)\n",
      "[t-SNE] Iteration 800: error = 0.7860990, gradient norm = 0.0001177 (50 iterations in 2.926s)\n",
      "[t-SNE] Iteration 850: error = 0.7797481, gradient norm = 0.0001058 (50 iterations in 2.925s)\n",
      "[t-SNE] Iteration 900: error = 0.7739398, gradient norm = 0.0001058 (50 iterations in 2.908s)\n",
      "[t-SNE] Iteration 950: error = 0.7692157, gradient norm = 0.0000981 (50 iterations in 2.994s)\n",
      "[t-SNE] Iteration 1000: error = 0.7647654, gradient norm = 0.0000994 (50 iterations in 2.931s)\n",
      "[t-SNE] Error after 1000 iterations: 0.764765\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.272s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.164220\n",
      "[t-SNE] Computed conditional probabilities in 0.129s\n",
      "[t-SNE] Iteration 50: error = 75.8464050, gradient norm = 0.0930489 (50 iterations in 5.006s)\n",
      "[t-SNE] Iteration 100: error = 62.3631516, gradient norm = 0.0361306 (50 iterations in 2.938s)\n",
      "[t-SNE] Iteration 150: error = 59.6494484, gradient norm = 0.0264778 (50 iterations in 2.806s)\n",
      "[t-SNE] Iteration 200: error = 58.4469109, gradient norm = 0.0227186 (50 iterations in 2.753s)\n",
      "[t-SNE] Iteration 250: error = 57.7311134, gradient norm = 0.0124070 (50 iterations in 2.711s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.731113\n",
      "[t-SNE] Iteration 300: error = 1.4271384, gradient norm = 0.0011430 (50 iterations in 2.961s)\n",
      "[t-SNE] Iteration 350: error = 1.1096569, gradient norm = 0.0004601 (50 iterations in 3.017s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 400: error = 0.9770809, gradient norm = 0.0003042 (50 iterations in 3.042s)\n",
      "[t-SNE] Iteration 450: error = 0.9085513, gradient norm = 0.0002326 (50 iterations in 3.063s)\n",
      "[t-SNE] Iteration 500: error = 0.8671668, gradient norm = 0.0001828 (50 iterations in 3.010s)\n",
      "[t-SNE] Iteration 550: error = 0.8402972, gradient norm = 0.0001574 (50 iterations in 3.004s)\n",
      "[t-SNE] Iteration 600: error = 0.8204641, gradient norm = 0.0001441 (50 iterations in 3.103s)\n",
      "[t-SNE] Iteration 650: error = 0.8057321, gradient norm = 0.0001277 (50 iterations in 3.030s)\n",
      "[t-SNE] Iteration 700: error = 0.7941699, gradient norm = 0.0001232 (50 iterations in 3.016s)\n",
      "[t-SNE] Iteration 750: error = 0.7853959, gradient norm = 0.0001117 (50 iterations in 3.056s)\n",
      "[t-SNE] Iteration 800: error = 0.7778537, gradient norm = 0.0001070 (50 iterations in 3.045s)\n",
      "[t-SNE] Iteration 850: error = 0.7711613, gradient norm = 0.0000989 (50 iterations in 3.020s)\n",
      "[t-SNE] Iteration 900: error = 0.7655066, gradient norm = 0.0001006 (50 iterations in 3.045s)\n",
      "[t-SNE] Iteration 950: error = 0.7604978, gradient norm = 0.0000914 (50 iterations in 3.029s)\n",
      "[t-SNE] Iteration 1000: error = 0.7556287, gradient norm = 0.0000861 (50 iterations in 3.075s)\n",
      "[t-SNE] Error after 1000 iterations: 0.755629\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.281s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.168057\n",
      "[t-SNE] Computed conditional probabilities in 0.132s\n",
      "[t-SNE] Iteration 50: error = 76.4641190, gradient norm = 0.0890725 (50 iterations in 4.474s)\n",
      "[t-SNE] Iteration 100: error = 62.7085381, gradient norm = 0.0303003 (50 iterations in 2.947s)\n",
      "[t-SNE] Iteration 150: error = 59.8208046, gradient norm = 0.0221987 (50 iterations in 2.792s)\n",
      "[t-SNE] Iteration 200: error = 58.5657234, gradient norm = 0.0179450 (50 iterations in 2.755s)\n",
      "[t-SNE] Iteration 250: error = 57.8281784, gradient norm = 0.0137167 (50 iterations in 2.792s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.828178\n",
      "[t-SNE] Iteration 300: error = 1.4274974, gradient norm = 0.0011298 (50 iterations in 2.901s)\n",
      "[t-SNE] Iteration 350: error = 1.1124176, gradient norm = 0.0004633 (50 iterations in 3.008s)\n",
      "[t-SNE] Iteration 400: error = 0.9817758, gradient norm = 0.0003061 (50 iterations in 2.966s)\n",
      "[t-SNE] Iteration 450: error = 0.9145055, gradient norm = 0.0002354 (50 iterations in 3.016s)\n",
      "[t-SNE] Iteration 500: error = 0.8739108, gradient norm = 0.0001976 (50 iterations in 3.065s)\n",
      "[t-SNE] Iteration 550: error = 0.8473817, gradient norm = 0.0001659 (50 iterations in 3.071s)\n",
      "[t-SNE] Iteration 600: error = 0.8289697, gradient norm = 0.0001477 (50 iterations in 3.055s)\n",
      "[t-SNE] Iteration 650: error = 0.8142725, gradient norm = 0.0001297 (50 iterations in 3.030s)\n",
      "[t-SNE] Iteration 700: error = 0.8022160, gradient norm = 0.0001224 (50 iterations in 3.032s)\n",
      "[t-SNE] Iteration 750: error = 0.7923575, gradient norm = 0.0001151 (50 iterations in 3.055s)\n",
      "[t-SNE] Iteration 800: error = 0.7845365, gradient norm = 0.0001118 (50 iterations in 3.025s)\n",
      "[t-SNE] Iteration 850: error = 0.7784756, gradient norm = 0.0000989 (50 iterations in 3.022s)\n",
      "[t-SNE] Iteration 900: error = 0.7725301, gradient norm = 0.0000923 (50 iterations in 3.030s)\n",
      "[t-SNE] Iteration 950: error = 0.7671605, gradient norm = 0.0000930 (50 iterations in 2.967s)\n",
      "[t-SNE] Iteration 1000: error = 0.7623328, gradient norm = 0.0000966 (50 iterations in 3.043s)\n",
      "[t-SNE] Error after 1000 iterations: 0.762333\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.295s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.194224\n",
      "[t-SNE] Computed conditional probabilities in 0.135s\n",
      "[t-SNE] Iteration 50: error = 76.1455460, gradient norm = 0.0887836 (50 iterations in 4.739s)\n",
      "[t-SNE] Iteration 100: error = 62.7334595, gradient norm = 0.0384212 (50 iterations in 2.929s)\n",
      "[t-SNE] Iteration 150: error = 59.9925537, gradient norm = 0.0195683 (50 iterations in 2.738s)\n",
      "[t-SNE] Iteration 200: error = 58.7688560, gradient norm = 0.0180132 (50 iterations in 2.755s)\n",
      "[t-SNE] Iteration 250: error = 58.0468941, gradient norm = 0.0157636 (50 iterations in 2.752s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.046894\n",
      "[t-SNE] Iteration 300: error = 1.4345193, gradient norm = 0.0011331 (50 iterations in 2.925s)\n",
      "[t-SNE] Iteration 350: error = 1.1220838, gradient norm = 0.0004734 (50 iterations in 3.074s)\n",
      "[t-SNE] Iteration 400: error = 0.9907842, gradient norm = 0.0003091 (50 iterations in 3.027s)\n",
      "[t-SNE] Iteration 450: error = 0.9224650, gradient norm = 0.0002395 (50 iterations in 3.099s)\n",
      "[t-SNE] Iteration 500: error = 0.8805671, gradient norm = 0.0001949 (50 iterations in 3.087s)\n",
      "[t-SNE] Iteration 550: error = 0.8528700, gradient norm = 0.0001678 (50 iterations in 3.045s)\n",
      "[t-SNE] Iteration 600: error = 0.8330365, gradient norm = 0.0001478 (50 iterations in 3.083s)\n",
      "[t-SNE] Iteration 650: error = 0.8177657, gradient norm = 0.0001306 (50 iterations in 3.036s)\n",
      "[t-SNE] Iteration 700: error = 0.8063424, gradient norm = 0.0001167 (50 iterations in 3.049s)\n",
      "[t-SNE] Iteration 750: error = 0.7967579, gradient norm = 0.0001164 (50 iterations in 3.042s)\n",
      "[t-SNE] Iteration 800: error = 0.7884507, gradient norm = 0.0001036 (50 iterations in 3.003s)\n",
      "[t-SNE] Iteration 850: error = 0.7810180, gradient norm = 0.0001034 (50 iterations in 3.038s)\n",
      "[t-SNE] Iteration 900: error = 0.7748393, gradient norm = 0.0001027 (50 iterations in 3.023s)\n",
      "[t-SNE] Iteration 950: error = 0.7698607, gradient norm = 0.0000917 (50 iterations in 3.044s)\n",
      "[t-SNE] Iteration 1000: error = 0.7648066, gradient norm = 0.0000912 (50 iterations in 3.036s)\n",
      "[t-SNE] Error after 1000 iterations: 0.764807\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.290s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.119742\n",
      "[t-SNE] Computed conditional probabilities in 0.134s\n",
      "[t-SNE] Iteration 50: error = 74.4486923, gradient norm = 0.0824765 (50 iterations in 5.517s)\n",
      "[t-SNE] Iteration 100: error = 62.7233543, gradient norm = 0.0321665 (50 iterations in 3.093s)\n",
      "[t-SNE] Iteration 150: error = 59.8769760, gradient norm = 0.0324736 (50 iterations in 2.755s)\n",
      "[t-SNE] Iteration 200: error = 58.7004662, gradient norm = 0.0254669 (50 iterations in 2.702s)\n",
      "[t-SNE] Iteration 250: error = 57.9983330, gradient norm = 0.0289463 (50 iterations in 2.691s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.998333\n",
      "[t-SNE] Iteration 300: error = 1.4408556, gradient norm = 0.0011837 (50 iterations in 2.970s)\n",
      "[t-SNE] Iteration 350: error = 1.1255361, gradient norm = 0.0004733 (50 iterations in 3.029s)\n",
      "[t-SNE] Iteration 400: error = 0.9940742, gradient norm = 0.0003110 (50 iterations in 3.018s)\n",
      "[t-SNE] Iteration 450: error = 0.9246570, gradient norm = 0.0002437 (50 iterations in 3.021s)\n",
      "[t-SNE] Iteration 500: error = 0.8827890, gradient norm = 0.0001891 (50 iterations in 3.013s)\n",
      "[t-SNE] Iteration 550: error = 0.8544616, gradient norm = 0.0001607 (50 iterations in 3.064s)\n",
      "[t-SNE] Iteration 600: error = 0.8341738, gradient norm = 0.0001417 (50 iterations in 3.070s)\n",
      "[t-SNE] Iteration 650: error = 0.8186054, gradient norm = 0.0001310 (50 iterations in 3.016s)\n",
      "[t-SNE] Iteration 700: error = 0.8067254, gradient norm = 0.0001222 (50 iterations in 3.014s)\n",
      "[t-SNE] Iteration 750: error = 0.7973380, gradient norm = 0.0001065 (50 iterations in 3.023s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 800: error = 0.7896518, gradient norm = 0.0001068 (50 iterations in 3.054s)\n",
      "[t-SNE] Iteration 850: error = 0.7830759, gradient norm = 0.0001042 (50 iterations in 3.015s)\n",
      "[t-SNE] Iteration 900: error = 0.7771019, gradient norm = 0.0000930 (50 iterations in 3.020s)\n",
      "[t-SNE] Iteration 950: error = 0.7715533, gradient norm = 0.0000909 (50 iterations in 3.016s)\n",
      "[t-SNE] Iteration 1000: error = 0.7664309, gradient norm = 0.0000901 (50 iterations in 2.982s)\n",
      "[t-SNE] Error after 1000 iterations: 0.766431\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.293s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.464029\n",
      "[t-SNE] Computed conditional probabilities in 0.133s\n",
      "[t-SNE] Iteration 50: error = 76.7244873, gradient norm = 0.0976580 (50 iterations in 4.711s)\n",
      "[t-SNE] Iteration 100: error = 62.7364960, gradient norm = 0.0335267 (50 iterations in 2.939s)\n",
      "[t-SNE] Iteration 150: error = 59.9608727, gradient norm = 0.0178631 (50 iterations in 2.934s)\n",
      "[t-SNE] Iteration 200: error = 58.7559280, gradient norm = 0.0256860 (50 iterations in 2.819s)\n",
      "[t-SNE] Iteration 250: error = 58.0427589, gradient norm = 0.0246523 (50 iterations in 2.815s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.042759\n",
      "[t-SNE] Iteration 300: error = 1.4334770, gradient norm = 0.0010972 (50 iterations in 2.943s)\n",
      "[t-SNE] Iteration 350: error = 1.1197155, gradient norm = 0.0004728 (50 iterations in 2.987s)\n",
      "[t-SNE] Iteration 400: error = 0.9880443, gradient norm = 0.0003089 (50 iterations in 3.018s)\n",
      "[t-SNE] Iteration 450: error = 0.9196278, gradient norm = 0.0002280 (50 iterations in 3.015s)\n",
      "[t-SNE] Iteration 500: error = 0.8776003, gradient norm = 0.0001903 (50 iterations in 3.026s)\n",
      "[t-SNE] Iteration 550: error = 0.8489919, gradient norm = 0.0001581 (50 iterations in 3.095s)\n",
      "[t-SNE] Iteration 600: error = 0.8279895, gradient norm = 0.0001405 (50 iterations in 3.036s)\n",
      "[t-SNE] Iteration 650: error = 0.8117836, gradient norm = 0.0001306 (50 iterations in 3.051s)\n",
      "[t-SNE] Iteration 700: error = 0.8001267, gradient norm = 0.0001190 (50 iterations in 3.088s)\n",
      "[t-SNE] Iteration 750: error = 0.7903319, gradient norm = 0.0001135 (50 iterations in 2.996s)\n",
      "[t-SNE] Iteration 800: error = 0.7820378, gradient norm = 0.0001098 (50 iterations in 3.047s)\n",
      "[t-SNE] Iteration 850: error = 0.7752497, gradient norm = 0.0000961 (50 iterations in 3.057s)\n",
      "[t-SNE] Iteration 900: error = 0.7694108, gradient norm = 0.0000962 (50 iterations in 3.053s)\n",
      "[t-SNE] Iteration 950: error = 0.7641777, gradient norm = 0.0000882 (50 iterations in 3.030s)\n",
      "[t-SNE] Iteration 1000: error = 0.7594341, gradient norm = 0.0000854 (50 iterations in 3.077s)\n",
      "[t-SNE] Error after 1000 iterations: 0.759434\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.293s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.060445\n",
      "[t-SNE] Computed conditional probabilities in 0.143s\n",
      "[t-SNE] Iteration 50: error = 77.2621689, gradient norm = 0.0914420 (50 iterations in 5.152s)\n",
      "[t-SNE] Iteration 100: error = 63.2425880, gradient norm = 0.0319431 (50 iterations in 3.001s)\n",
      "[t-SNE] Iteration 150: error = 60.3792191, gradient norm = 0.0266204 (50 iterations in 2.821s)\n",
      "[t-SNE] Iteration 200: error = 59.1038589, gradient norm = 0.0184910 (50 iterations in 2.772s)\n",
      "[t-SNE] Iteration 250: error = 58.3750648, gradient norm = 0.0207852 (50 iterations in 2.765s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.375065\n",
      "[t-SNE] Iteration 300: error = 1.4602706, gradient norm = 0.0011687 (50 iterations in 2.988s)\n",
      "[t-SNE] Iteration 350: error = 1.1370729, gradient norm = 0.0004895 (50 iterations in 3.064s)\n",
      "[t-SNE] Iteration 400: error = 1.0005940, gradient norm = 0.0003268 (50 iterations in 3.082s)\n",
      "[t-SNE] Iteration 450: error = 0.9294263, gradient norm = 0.0002526 (50 iterations in 3.105s)\n",
      "[t-SNE] Iteration 500: error = 0.8862800, gradient norm = 0.0002023 (50 iterations in 3.104s)\n",
      "[t-SNE] Iteration 550: error = 0.8567129, gradient norm = 0.0001719 (50 iterations in 3.105s)\n",
      "[t-SNE] Iteration 600: error = 0.8349624, gradient norm = 0.0001528 (50 iterations in 3.043s)\n",
      "[t-SNE] Iteration 650: error = 0.8185107, gradient norm = 0.0001361 (50 iterations in 3.093s)\n",
      "[t-SNE] Iteration 700: error = 0.8056893, gradient norm = 0.0001319 (50 iterations in 3.063s)\n",
      "[t-SNE] Iteration 750: error = 0.7959070, gradient norm = 0.0001141 (50 iterations in 3.040s)\n",
      "[t-SNE] Iteration 800: error = 0.7873445, gradient norm = 0.0001135 (50 iterations in 3.055s)\n",
      "[t-SNE] Iteration 850: error = 0.7801845, gradient norm = 0.0001085 (50 iterations in 3.069s)\n",
      "[t-SNE] Iteration 900: error = 0.7741162, gradient norm = 0.0001027 (50 iterations in 3.053s)\n",
      "[t-SNE] Iteration 950: error = 0.7687368, gradient norm = 0.0000956 (50 iterations in 3.078s)\n",
      "[t-SNE] Iteration 1000: error = 0.7638866, gradient norm = 0.0000961 (50 iterations in 3.068s)\n",
      "[t-SNE] Error after 1000 iterations: 0.763887\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.350s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.076953\n",
      "[t-SNE] Computed conditional probabilities in 0.143s\n",
      "[t-SNE] Iteration 50: error = 77.0912704, gradient norm = 0.0933988 (50 iterations in 5.691s)\n",
      "[t-SNE] Iteration 100: error = 63.2720146, gradient norm = 0.0322943 (50 iterations in 3.019s)\n",
      "[t-SNE] Iteration 150: error = 60.4324608, gradient norm = 0.0193706 (50 iterations in 2.868s)\n",
      "[t-SNE] Iteration 200: error = 59.2079964, gradient norm = 0.0215727 (50 iterations in 2.791s)\n",
      "[t-SNE] Iteration 250: error = 58.4884186, gradient norm = 0.0129956 (50 iterations in 2.752s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.488419\n",
      "[t-SNE] Iteration 300: error = 1.4581579, gradient norm = 0.0011426 (50 iterations in 2.969s)\n",
      "[t-SNE] Iteration 350: error = 1.1360806, gradient norm = 0.0004994 (50 iterations in 3.107s)\n",
      "[t-SNE] Iteration 400: error = 1.0001882, gradient norm = 0.0003356 (50 iterations in 3.048s)\n",
      "[t-SNE] Iteration 450: error = 0.9297808, gradient norm = 0.0002459 (50 iterations in 3.066s)\n",
      "[t-SNE] Iteration 500: error = 0.8855035, gradient norm = 0.0002042 (50 iterations in 3.065s)\n",
      "[t-SNE] Iteration 550: error = 0.8550594, gradient norm = 0.0001764 (50 iterations in 3.092s)\n",
      "[t-SNE] Iteration 600: error = 0.8339919, gradient norm = 0.0001545 (50 iterations in 3.051s)\n",
      "[t-SNE] Iteration 650: error = 0.8178737, gradient norm = 0.0001386 (50 iterations in 3.034s)\n",
      "[t-SNE] Iteration 700: error = 0.8052393, gradient norm = 0.0001260 (50 iterations in 3.064s)\n",
      "[t-SNE] Iteration 750: error = 0.7947223, gradient norm = 0.0001232 (50 iterations in 3.060s)\n",
      "[t-SNE] Iteration 800: error = 0.7856891, gradient norm = 0.0001151 (50 iterations in 3.054s)\n",
      "[t-SNE] Iteration 850: error = 0.7783082, gradient norm = 0.0001033 (50 iterations in 3.064s)\n",
      "[t-SNE] Iteration 900: error = 0.7715107, gradient norm = 0.0001057 (50 iterations in 3.025s)\n",
      "[t-SNE] Iteration 950: error = 0.7659073, gradient norm = 0.0001041 (50 iterations in 3.034s)\n",
      "[t-SNE] Iteration 1000: error = 0.7616569, gradient norm = 0.0000950 (50 iterations in 3.015s)\n",
      "[t-SNE] Error after 1000 iterations: 0.761657\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.004s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.325s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.107501\n",
      "[t-SNE] Computed conditional probabilities in 0.159s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 50: error = 75.7864380, gradient norm = 0.1006001 (50 iterations in 4.356s)\n",
      "[t-SNE] Iteration 100: error = 63.3646088, gradient norm = 0.0346929 (50 iterations in 3.047s)\n",
      "[t-SNE] Iteration 150: error = 60.6022530, gradient norm = 0.0232958 (50 iterations in 2.863s)\n",
      "[t-SNE] Iteration 200: error = 59.3856049, gradient norm = 0.0183071 (50 iterations in 2.812s)\n",
      "[t-SNE] Iteration 250: error = 58.6777344, gradient norm = 0.0155249 (50 iterations in 2.802s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.677734\n",
      "[t-SNE] Iteration 300: error = 1.4557879, gradient norm = 0.0011881 (50 iterations in 3.003s)\n",
      "[t-SNE] Iteration 350: error = 1.1361359, gradient norm = 0.0004977 (50 iterations in 3.086s)\n",
      "[t-SNE] Iteration 400: error = 1.0005935, gradient norm = 0.0003349 (50 iterations in 3.041s)\n",
      "[t-SNE] Iteration 450: error = 0.9293342, gradient norm = 0.0002553 (50 iterations in 3.085s)\n",
      "[t-SNE] Iteration 500: error = 0.8854395, gradient norm = 0.0002037 (50 iterations in 3.074s)\n",
      "[t-SNE] Iteration 550: error = 0.8558397, gradient norm = 0.0001710 (50 iterations in 3.119s)\n",
      "[t-SNE] Iteration 600: error = 0.8340902, gradient norm = 0.0001515 (50 iterations in 3.093s)\n",
      "[t-SNE] Iteration 650: error = 0.8179302, gradient norm = 0.0001409 (50 iterations in 3.097s)\n",
      "[t-SNE] Iteration 700: error = 0.8061202, gradient norm = 0.0001337 (50 iterations in 3.138s)\n",
      "[t-SNE] Iteration 750: error = 0.7974638, gradient norm = 0.0001251 (50 iterations in 3.104s)\n",
      "[t-SNE] Iteration 800: error = 0.7897273, gradient norm = 0.0001106 (50 iterations in 3.135s)\n",
      "[t-SNE] Iteration 850: error = 0.7827095, gradient norm = 0.0001067 (50 iterations in 3.137s)\n",
      "[t-SNE] Iteration 900: error = 0.7762259, gradient norm = 0.0001014 (50 iterations in 3.107s)\n",
      "[t-SNE] Iteration 950: error = 0.7705279, gradient norm = 0.0000972 (50 iterations in 3.126s)\n",
      "[t-SNE] Iteration 1000: error = 0.7654948, gradient norm = 0.0000936 (50 iterations in 3.106s)\n",
      "[t-SNE] Error after 1000 iterations: 0.765495\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.326s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.335702\n",
      "[t-SNE] Computed conditional probabilities in 0.142s\n",
      "[t-SNE] Iteration 50: error = 76.1213913, gradient norm = 0.1029925 (50 iterations in 5.236s)\n",
      "[t-SNE] Iteration 100: error = 63.1168785, gradient norm = 0.0299003 (50 iterations in 2.961s)\n",
      "[t-SNE] Iteration 150: error = 60.4143791, gradient norm = 0.0179254 (50 iterations in 2.803s)\n",
      "[t-SNE] Iteration 200: error = 59.2480850, gradient norm = 0.0233915 (50 iterations in 2.740s)\n",
      "[t-SNE] Iteration 250: error = 58.5618248, gradient norm = 0.0220301 (50 iterations in 2.742s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.561825\n",
      "[t-SNE] Iteration 300: error = 1.4575837, gradient norm = 0.0011632 (50 iterations in 3.008s)\n",
      "[t-SNE] Iteration 350: error = 1.1348979, gradient norm = 0.0004883 (50 iterations in 3.061s)\n",
      "[t-SNE] Iteration 400: error = 0.9990783, gradient norm = 0.0003259 (50 iterations in 3.053s)\n",
      "[t-SNE] Iteration 450: error = 0.9275683, gradient norm = 0.0002626 (50 iterations in 3.096s)\n",
      "[t-SNE] Iteration 500: error = 0.8838518, gradient norm = 0.0002093 (50 iterations in 3.125s)\n",
      "[t-SNE] Iteration 550: error = 0.8541216, gradient norm = 0.0001690 (50 iterations in 3.188s)\n",
      "[t-SNE] Iteration 600: error = 0.8323544, gradient norm = 0.0001552 (50 iterations in 3.172s)\n",
      "[t-SNE] Iteration 650: error = 0.8159372, gradient norm = 0.0001340 (50 iterations in 3.138s)\n",
      "[t-SNE] Iteration 700: error = 0.8034395, gradient norm = 0.0001221 (50 iterations in 3.168s)\n",
      "[t-SNE] Iteration 750: error = 0.7930575, gradient norm = 0.0001152 (50 iterations in 3.132s)\n",
      "[t-SNE] Iteration 800: error = 0.7842669, gradient norm = 0.0001192 (50 iterations in 3.167s)\n",
      "[t-SNE] Iteration 850: error = 0.7775013, gradient norm = 0.0001110 (50 iterations in 3.179s)\n",
      "[t-SNE] Iteration 900: error = 0.7717017, gradient norm = 0.0001036 (50 iterations in 3.110s)\n",
      "[t-SNE] Iteration 950: error = 0.7661569, gradient norm = 0.0000959 (50 iterations in 3.093s)\n",
      "[t-SNE] Iteration 1000: error = 0.7609296, gradient norm = 0.0000952 (50 iterations in 3.073s)\n",
      "[t-SNE] Error after 1000 iterations: 0.760930\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.345s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.547406\n",
      "[t-SNE] Computed conditional probabilities in 0.140s\n",
      "[t-SNE] Iteration 50: error = 77.5951080, gradient norm = 0.0772822 (50 iterations in 4.969s)\n",
      "[t-SNE] Iteration 100: error = 63.4428139, gradient norm = 0.0323945 (50 iterations in 2.976s)\n",
      "[t-SNE] Iteration 150: error = 60.6113129, gradient norm = 0.0229149 (50 iterations in 2.797s)\n",
      "[t-SNE] Iteration 200: error = 59.3811111, gradient norm = 0.0197564 (50 iterations in 2.785s)\n",
      "[t-SNE] Iteration 250: error = 58.6649780, gradient norm = 0.0153271 (50 iterations in 2.778s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.664978\n",
      "[t-SNE] Iteration 300: error = 1.4512492, gradient norm = 0.0011602 (50 iterations in 2.954s)\n",
      "[t-SNE] Iteration 350: error = 1.1310272, gradient norm = 0.0004822 (50 iterations in 3.052s)\n",
      "[t-SNE] Iteration 400: error = 0.9943661, gradient norm = 0.0003218 (50 iterations in 3.059s)\n",
      "[t-SNE] Iteration 450: error = 0.9231835, gradient norm = 0.0002453 (50 iterations in 3.049s)\n",
      "[t-SNE] Iteration 500: error = 0.8792434, gradient norm = 0.0001979 (50 iterations in 3.037s)\n",
      "[t-SNE] Iteration 550: error = 0.8501945, gradient norm = 0.0001689 (50 iterations in 3.071s)\n",
      "[t-SNE] Iteration 600: error = 0.8292989, gradient norm = 0.0001476 (50 iterations in 3.034s)\n",
      "[t-SNE] Iteration 650: error = 0.8131753, gradient norm = 0.0001357 (50 iterations in 3.012s)\n",
      "[t-SNE] Iteration 700: error = 0.8010461, gradient norm = 0.0001234 (50 iterations in 3.030s)\n",
      "[t-SNE] Iteration 750: error = 0.7909516, gradient norm = 0.0001178 (50 iterations in 3.032s)\n",
      "[t-SNE] Iteration 800: error = 0.7825410, gradient norm = 0.0001091 (50 iterations in 3.061s)\n",
      "[t-SNE] Iteration 850: error = 0.7755983, gradient norm = 0.0001021 (50 iterations in 3.062s)\n",
      "[t-SNE] Iteration 900: error = 0.7696509, gradient norm = 0.0000995 (50 iterations in 3.085s)\n",
      "[t-SNE] Iteration 950: error = 0.7643028, gradient norm = 0.0001020 (50 iterations in 3.079s)\n",
      "[t-SNE] Iteration 1000: error = 0.7600259, gradient norm = 0.0000945 (50 iterations in 3.051s)\n",
      "[t-SNE] Error after 1000 iterations: 0.760026\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.344s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.632438\n",
      "[t-SNE] Computed conditional probabilities in 0.139s\n",
      "[t-SNE] Iteration 50: error = 78.1709900, gradient norm = 0.1011081 (50 iterations in 4.812s)\n",
      "[t-SNE] Iteration 100: error = 63.3692627, gradient norm = 0.0337172 (50 iterations in 2.939s)\n",
      "[t-SNE] Iteration 150: error = 60.5954971, gradient norm = 0.0240253 (50 iterations in 2.753s)\n",
      "[t-SNE] Iteration 200: error = 59.4146080, gradient norm = 0.0159605 (50 iterations in 2.765s)\n",
      "[t-SNE] Iteration 250: error = 58.7196159, gradient norm = 0.0177438 (50 iterations in 2.729s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.719616\n",
      "[t-SNE] Iteration 300: error = 1.4581268, gradient norm = 0.0011660 (50 iterations in 2.981s)\n",
      "[t-SNE] Iteration 350: error = 1.1349149, gradient norm = 0.0004781 (50 iterations in 3.047s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 400: error = 0.9991807, gradient norm = 0.0003285 (50 iterations in 3.057s)\n",
      "[t-SNE] Iteration 450: error = 0.9280047, gradient norm = 0.0002457 (50 iterations in 3.053s)\n",
      "[t-SNE] Iteration 500: error = 0.8854284, gradient norm = 0.0001950 (50 iterations in 3.051s)\n",
      "[t-SNE] Iteration 550: error = 0.8560243, gradient norm = 0.0001694 (50 iterations in 3.111s)\n",
      "[t-SNE] Iteration 600: error = 0.8342892, gradient norm = 0.0001495 (50 iterations in 3.095s)\n",
      "[t-SNE] Iteration 650: error = 0.8174831, gradient norm = 0.0001349 (50 iterations in 3.091s)\n",
      "[t-SNE] Iteration 700: error = 0.8042337, gradient norm = 0.0001266 (50 iterations in 3.092s)\n",
      "[t-SNE] Iteration 750: error = 0.7942560, gradient norm = 0.0001113 (50 iterations in 3.078s)\n",
      "[t-SNE] Iteration 800: error = 0.7854857, gradient norm = 0.0001081 (50 iterations in 3.109s)\n",
      "[t-SNE] Iteration 850: error = 0.7780178, gradient norm = 0.0000998 (50 iterations in 3.098s)\n",
      "[t-SNE] Iteration 900: error = 0.7713808, gradient norm = 0.0000955 (50 iterations in 3.128s)\n",
      "[t-SNE] Iteration 950: error = 0.7652678, gradient norm = 0.0000947 (50 iterations in 3.093s)\n",
      "[t-SNE] Iteration 1000: error = 0.7597721, gradient norm = 0.0000846 (50 iterations in 3.145s)\n",
      "[t-SNE] Error after 1000 iterations: 0.759772\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3450 samples in 0.003s...\n",
      "[t-SNE] Computed neighbors for 3450 samples in 0.381s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3450\n",
      "[t-SNE] Computed conditional probabilities for sample 3450 / 3450\n",
      "[t-SNE] Mean sigma: 5.512912\n",
      "[t-SNE] Computed conditional probabilities in 0.163s\n",
      "[t-SNE] Iteration 50: error = 76.6449127, gradient norm = 0.0846867 (50 iterations in 4.841s)\n",
      "[t-SNE] Iteration 100: error = 63.4822578, gradient norm = 0.0309238 (50 iterations in 3.009s)\n",
      "[t-SNE] Iteration 150: error = 60.7679977, gradient norm = 0.0209339 (50 iterations in 2.823s)\n",
      "[t-SNE] Iteration 200: error = 59.6070557, gradient norm = 0.0239212 (50 iterations in 2.828s)\n",
      "[t-SNE] Iteration 250: error = 58.9412155, gradient norm = 0.0203277 (50 iterations in 2.786s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.941216\n",
      "[t-SNE] Iteration 300: error = 1.4657036, gradient norm = 0.0011555 (50 iterations in 3.088s)\n",
      "[t-SNE] Iteration 350: error = 1.1423662, gradient norm = 0.0004897 (50 iterations in 3.143s)\n",
      "[t-SNE] Iteration 400: error = 1.0053315, gradient norm = 0.0003283 (50 iterations in 3.102s)\n",
      "[t-SNE] Iteration 450: error = 0.9330891, gradient norm = 0.0002471 (50 iterations in 3.238s)\n",
      "[t-SNE] Iteration 500: error = 0.8890764, gradient norm = 0.0002089 (50 iterations in 3.218s)\n",
      "[t-SNE] Iteration 550: error = 0.8588554, gradient norm = 0.0001756 (50 iterations in 3.217s)\n",
      "[t-SNE] Iteration 600: error = 0.8365281, gradient norm = 0.0001532 (50 iterations in 3.262s)\n",
      "[t-SNE] Iteration 650: error = 0.8196524, gradient norm = 0.0001350 (50 iterations in 3.227s)\n",
      "[t-SNE] Iteration 700: error = 0.8058651, gradient norm = 0.0001251 (50 iterations in 3.409s)\n",
      "[t-SNE] Iteration 750: error = 0.7950521, gradient norm = 0.0001106 (50 iterations in 3.314s)\n",
      "[t-SNE] Iteration 800: error = 0.7856991, gradient norm = 0.0001121 (50 iterations in 3.119s)\n",
      "[t-SNE] Iteration 850: error = 0.7779652, gradient norm = 0.0001040 (50 iterations in 3.141s)\n",
      "[t-SNE] Iteration 900: error = 0.7720954, gradient norm = 0.0000939 (50 iterations in 3.153s)\n",
      "[t-SNE] Iteration 950: error = 0.7664752, gradient norm = 0.0000921 (50 iterations in 3.134s)\n",
      "[t-SNE] Iteration 1000: error = 0.7616557, gradient norm = 0.0000865 (50 iterations in 3.478s)\n",
      "[t-SNE] Error after 1000 iterations: 0.761656\n"
     ]
    }
   ],
   "source": [
    "# Store ever tenth iteration with a fixed point\n",
    "storage_dir = '/Volumes/Stockage/alex/1000G/iterations'\n",
    "pc_list = [i for i in range(2,50)]# + [i for i in range(20,51,5)]\n",
    "\n",
    "for pc in pc_list:\n",
    "    comps = pca_full.components_[:pc,:]\n",
    "    projections_to_PCs = np.dot((transposed_genotype_matrix - pca_full.mean_), comps.T)\n",
    "    \n",
    "    tsne_proj = TSNE(n_components=2, verbose=3, random_state=1).fit_transform(projections_to_PCs)\n",
    "    \n",
    "    np.savetxt(os.path.join(storage_dir,str(pc).zfill(2)),tsne_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n",
      "03\n",
      "04\n",
      "05\n",
      "06\n",
      "07\n",
      "08\n",
      "09\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(storage_dir):\n",
    "    if os.path.isfile(os.path.join(storage_dir,file)):\n",
    "        \n",
    "        temp_proj = np.loadtxt(os.path.join(storage_dir,file))\n",
    "        # Plot images for each of the projections.\n",
    "        component_1_id = 0\n",
    "        component_2_id = 1\n",
    "        \n",
    "        fig = plt.figure(figsize=(30,30))\n",
    "        ax = fig.add_subplot(111, aspect=1)\n",
    "\n",
    "        p = figure(plot_width=1500, plot_height=800)\n",
    "        p.title.text = \"test\"\n",
    "        \n",
    "        print(file)\n",
    "\n",
    "        for cont in continents: \n",
    "            for pop in pop_by_continent[cont]:\n",
    "                projections_within_population = temp_proj[indices_of_population_members[pop]]\n",
    "                p.circle(projections_within_population[:,component_1_id], projections_within_population[:,component_2_id], \n",
    "                         legend=name_by_code[pop], color = color_dict[pop])\n",
    "                ax.set_title('Iteration: ' + file)\n",
    "                ax.plot(projections_within_population[:,component_1_id], projections_within_population[:,component_2_id],\n",
    "                        '.',color=color_dict[pop])\n",
    "\n",
    "        p.legend.location = \"top_left\"\n",
    "\n",
    "        p.legend.click_policy=\"hide\"\n",
    "\n",
    "        fig.savefig(\"/Volumes/Stockage/alex/1000G/iterations/Images/\"+str(file).zfill(2)+\".jpeg\",format='jpeg')\n",
    "        plt.close()\n",
    "        \n",
    "        output_file(\"/Volumes/Stockage/alex/1000G/iterations/HTML/\"+str(file).zfill(2)+\".html\", title=\"test\")\n",
    "\n",
    "        save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Animate the images\n",
    "import imageio\n",
    "\n",
    "img_dir = '/Volumes/Stockage/alex/1000G/iterations/Images'\n",
    "movie_dir = '/Volumes/Stockage/alex/1000G/iterations/Movies'\n",
    "movie_name = '1000G_animation.gif'\n",
    "    \n",
    "images = []\n",
    "for filename in os.listdir(img_dir):\n",
    "    if os.path.isdir(os.path.join(img_dir,filename)):\n",
    "        continue\n",
    "        \n",
    "    images.append(imageio.imread(os.path.join(img_dir,filename)))\n",
    "\n",
    "imageio.mimsave(os.path.join(movie_dir,movie_name), images,duration=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing time differences below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r hrs_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tsne_custom = TSNE_custom(n_components=2).fit_transform(pca_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to complete Scikit-learn t-SNE: 382.89354395866394\n",
      "Time to complete multicore t-SNE: 124.2505190372467\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "tsne_sklearn = TSNE(n_components=2).fit_transform(hrs_data_array[:,0:20])\n",
    "t1 = time.time()\n",
    "\n",
    "print('Time to complete Scikit-learn t-SNE: ' + str(t1-t0))\n",
    "\n",
    "t2 = time.time()\n",
    "tsne_mcore = mTSNE(n_components=2, n_jobs=4).fit_transform(hrs_data_array[:,0:20])\n",
    "t3 = time.time()\n",
    "\n",
    "print('Time to complete multicore t-SNE: ' + str(t3-t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to complete multicore t-SNE (8 jobs): 137.14015793800354\n"
     ]
    }
   ],
   "source": [
    "t2 = time.time()\n",
    "tsne_mcore2 = mTSNE(n_components=2, n_jobs=8).fit_transform(hrs_data_array[:,0:20])\n",
    "t3 = time.time()\n",
    "\n",
    "print('Time to complete multicore t-SNE (8 jobs): ' + str(t3-t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_array = np.loadtxt('/Users/alex/Documents/Ethnicity/animation/test_dir/999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x108ed7b00>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1cba4390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(temp_array[:,0],temp_array[:,1],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test UKBB+1000G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ukbb_1000g = np.loadtxt('/Volumes/Stockage/alex/ukbb_projections/ukbb_1000g_tsne_pc10_plex_iter_10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/alex/Documents/Ethnicity/Alex_UKBB_KGP') as contents:\n",
    "    ukbb_1000g_aux = contents.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux_data = []\n",
    "for u in ukbb_1000g_aux[0:]:\n",
    "    aux_data.append([u_.strip('\\\"\\n') for u_ in u.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>IID</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000011</td>\n",
       "      <td>1000011</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000029</td>\n",
       "      <td>1000029</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000032</td>\n",
       "      <td>1000032</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000048</td>\n",
       "      <td>1000048</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000057</td>\n",
       "      <td>1000057</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000064</td>\n",
       "      <td>1000064</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000073</td>\n",
       "      <td>1000073</td>\n",
       "      <td>Any other white Background</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000080</td>\n",
       "      <td>1000080</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000095</td>\n",
       "      <td>1000095</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000102</td>\n",
       "      <td>1000102</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1000119</td>\n",
       "      <td>1000119</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1000121</td>\n",
       "      <td>1000121</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000136</td>\n",
       "      <td>1000136</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1000140</td>\n",
       "      <td>1000140</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1000154</td>\n",
       "      <td>1000154</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1000167</td>\n",
       "      <td>1000167</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1000175</td>\n",
       "      <td>1000175</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1000188</td>\n",
       "      <td>1000188</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000193</td>\n",
       "      <td>1000193</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1000204</td>\n",
       "      <td>1000204</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1000215</td>\n",
       "      <td>1000215</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1000227</td>\n",
       "      <td>1000227</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1000238</td>\n",
       "      <td>1000238</td>\n",
       "      <td>Pakistani</td>\n",
       "      <td>ASIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1000243</td>\n",
       "      <td>1000243</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1000252</td>\n",
       "      <td>1000252</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1000261</td>\n",
       "      <td>1000261</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1000279</td>\n",
       "      <td>1000279</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1000286</td>\n",
       "      <td>1000286</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1000290</td>\n",
       "      <td>1000290</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1000307</td>\n",
       "      <td>1000307</td>\n",
       "      <td>British</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489692</th>\n",
       "      <td>HG02427</td>\n",
       "      <td>HG02427</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489693</th>\n",
       "      <td>HG02428</td>\n",
       "      <td>HG02428</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489694</th>\n",
       "      <td>HG02429</td>\n",
       "      <td>HG02429</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489695</th>\n",
       "      <td>HG02433</td>\n",
       "      <td>HG02433</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489696</th>\n",
       "      <td>HG02436</td>\n",
       "      <td>HG02436</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489697</th>\n",
       "      <td>HG02442</td>\n",
       "      <td>HG02442</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489698</th>\n",
       "      <td>HG02445</td>\n",
       "      <td>HG02445</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489699</th>\n",
       "      <td>HG02449</td>\n",
       "      <td>HG02449</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489700</th>\n",
       "      <td>HG02450</td>\n",
       "      <td>HG02450</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489701</th>\n",
       "      <td>HG02451</td>\n",
       "      <td>HG02451</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489702</th>\n",
       "      <td>HG02470</td>\n",
       "      <td>HG02470</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489703</th>\n",
       "      <td>HG02471</td>\n",
       "      <td>HG02471</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489704</th>\n",
       "      <td>HG02478</td>\n",
       "      <td>HG02478</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489705</th>\n",
       "      <td>HG02479</td>\n",
       "      <td>HG02479</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489706</th>\n",
       "      <td>HG02480</td>\n",
       "      <td>HG02480</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489707</th>\n",
       "      <td>HG02484</td>\n",
       "      <td>HG02484</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489708</th>\n",
       "      <td>HG02485</td>\n",
       "      <td>HG02485</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489709</th>\n",
       "      <td>HG02486</td>\n",
       "      <td>HG02486</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489710</th>\n",
       "      <td>HG02489</td>\n",
       "      <td>HG02489</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489711</th>\n",
       "      <td>HG02496</td>\n",
       "      <td>HG02496</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489712</th>\n",
       "      <td>HG02497</td>\n",
       "      <td>HG02497</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489713</th>\n",
       "      <td>HG02508</td>\n",
       "      <td>HG02508</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489714</th>\n",
       "      <td>HG02511</td>\n",
       "      <td>HG02511</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489715</th>\n",
       "      <td>HG02512</td>\n",
       "      <td>HG02512</td>\n",
       "      <td>KHV</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489716</th>\n",
       "      <td>HG02513</td>\n",
       "      <td>HG02513</td>\n",
       "      <td>KHV</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489717</th>\n",
       "      <td>HG02514</td>\n",
       "      <td>HG02514</td>\n",
       "      <td>KHV</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489718</th>\n",
       "      <td>HG02521</td>\n",
       "      <td>HG02521</td>\n",
       "      <td>KHV</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489719</th>\n",
       "      <td>HG02522</td>\n",
       "      <td>HG02522</td>\n",
       "      <td>KHV</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489720</th>\n",
       "      <td>HG02523</td>\n",
       "      <td>HG02523</td>\n",
       "      <td>KHV</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489721</th>\n",
       "      <td>HG02537</td>\n",
       "      <td>HG02537</td>\n",
       "      <td>ACB</td>\n",
       "      <td>AFR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>489722 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FID      IID                   Ethnicity  group\n",
       "0       1000011  1000011                     British  WHITE\n",
       "1       1000029  1000029                     British  WHITE\n",
       "2       1000032  1000032                     British  WHITE\n",
       "3       1000048  1000048                     British  WHITE\n",
       "4       1000057  1000057                     British  WHITE\n",
       "5       1000064  1000064                     British  WHITE\n",
       "6       1000073  1000073  Any other white Background  WHITE\n",
       "7       1000080  1000080                     British  WHITE\n",
       "8       1000095  1000095                     British  WHITE\n",
       "9       1000102  1000102                     British  WHITE\n",
       "10      1000119  1000119                     British  WHITE\n",
       "11      1000121  1000121                     British  WHITE\n",
       "12      1000136  1000136                     British  WHITE\n",
       "13      1000140  1000140                     British  WHITE\n",
       "14      1000154  1000154                     British  WHITE\n",
       "15      1000167  1000167                     British  WHITE\n",
       "16      1000175  1000175                     British  WHITE\n",
       "17      1000188  1000188                     British  WHITE\n",
       "18      1000193  1000193                     British  WHITE\n",
       "19      1000204  1000204                     British  WHITE\n",
       "20      1000215  1000215                     British  WHITE\n",
       "21      1000227  1000227                     British  WHITE\n",
       "22      1000238  1000238                   Pakistani  ASIAN\n",
       "23      1000243  1000243                     British  WHITE\n",
       "24      1000252  1000252                     British  WHITE\n",
       "25      1000261  1000261                     British  WHITE\n",
       "26      1000279  1000279                     British  WHITE\n",
       "27      1000286  1000286                     British  WHITE\n",
       "28      1000290  1000290                     British  WHITE\n",
       "29      1000307  1000307                     British  WHITE\n",
       "...         ...      ...                         ...    ...\n",
       "489692  HG02427  HG02427                         ACB    AFR\n",
       "489693  HG02428  HG02428                         ACB    AFR\n",
       "489694  HG02429  HG02429                         ACB    AFR\n",
       "489695  HG02433  HG02433                         ACB    AFR\n",
       "489696  HG02436  HG02436                         ACB    AFR\n",
       "489697  HG02442  HG02442                         ACB    AFR\n",
       "489698  HG02445  HG02445                         ACB    AFR\n",
       "489699  HG02449  HG02449                         ACB    AFR\n",
       "489700  HG02450  HG02450                         ACB    AFR\n",
       "489701  HG02451  HG02451                         ACB    AFR\n",
       "489702  HG02470  HG02470                         ACB    AFR\n",
       "489703  HG02471  HG02471                         ACB    AFR\n",
       "489704  HG02478  HG02478                         ACB    AFR\n",
       "489705  HG02479  HG02479                         ACB    AFR\n",
       "489706  HG02480  HG02480                         ACB    AFR\n",
       "489707  HG02484  HG02484                         ACB    AFR\n",
       "489708  HG02485  HG02485                         ACB    AFR\n",
       "489709  HG02486  HG02486                         ACB    AFR\n",
       "489710  HG02489  HG02489                         ACB    AFR\n",
       "489711  HG02496  HG02496                         ACB    AFR\n",
       "489712  HG02497  HG02497                         ACB    AFR\n",
       "489713  HG02508  HG02508                         ACB    AFR\n",
       "489714  HG02511  HG02511                         ACB    AFR\n",
       "489715  HG02512  HG02512                         KHV    SAS\n",
       "489716  HG02513  HG02513                         KHV    SAS\n",
       "489717  HG02514  HG02514                         KHV    SAS\n",
       "489718  HG02521  HG02521                         KHV    SAS\n",
       "489719  HG02522  HG02522                         KHV    SAS\n",
       "489720  HG02523  HG02523                         KHV    SAS\n",
       "489721  HG02537  HG02537                         ACB    AFR\n",
       "\n",
       "[489722 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data frames for management of ethnicity\n",
    "aux_data_df = pd.DataFrame.from_records(aux_data[1:],columns=aux_data[0])\n",
    "aux_data_df = aux_data_df[['FID','IID','Ethnicity','group']]\n",
    "aux_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>IID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACB</th>\n",
       "      <th>AFR</th>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASW</th>\n",
       "      <th>AFR</th>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African</th>\n",
       "      <th>BLACK</th>\n",
       "      <td>3205</td>\n",
       "      <td>3205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Any other Asian background</th>\n",
       "      <th>ASIAN</th>\n",
       "      <td>1747</td>\n",
       "      <td>1747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Any other Black background</th>\n",
       "      <th>BLACK</th>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Any other Mixed background</th>\n",
       "      <th>MIXED</th>\n",
       "      <td>996</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Any other white Background</th>\n",
       "      <th>WHITE</th>\n",
       "      <td>15760</td>\n",
       "      <td>15760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian or Asian British</th>\n",
       "      <th>ASIAN</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bangladeshi</th>\n",
       "      <th>ASIAN</th>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black or Black Britsh</th>\n",
       "      <th>BLACK</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>British</th>\n",
       "      <th>WHITE</th>\n",
       "      <td>430307</td>\n",
       "      <td>430307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDX</th>\n",
       "      <th>EAS</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEU</th>\n",
       "      <th>EUR</th>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHB</th>\n",
       "      <th>EAS</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHD</th>\n",
       "      <th>EAS</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHS</th>\n",
       "      <th>EAS</th>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLM</th>\n",
       "      <th>AMR</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caribbean</th>\n",
       "      <th>BLACK</th>\n",
       "      <td>4299</td>\n",
       "      <td>4299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <th>CHINESE</th>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Do notknow</th>\n",
       "      <th>OTHER</th>\n",
       "      <td>204</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIN</th>\n",
       "      <th>EUR</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBR</th>\n",
       "      <th>EUR</th>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GIH</th>\n",
       "      <th>SAS</th>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBS</th>\n",
       "      <th>EUR</th>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <th>ASIAN</th>\n",
       "      <td>5660</td>\n",
       "      <td>5660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irish</th>\n",
       "      <th>WHITE</th>\n",
       "      <td>12719</td>\n",
       "      <td>12719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPT</th>\n",
       "      <th>EAS</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KHV</th>\n",
       "      <th>SAS</th>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LWK</th>\n",
       "      <th>AFR</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MKK</th>\n",
       "      <th>AFR</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MXL</th>\n",
       "      <th>AMR</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixed</th>\n",
       "      <th>MIXED</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Ethnicity</th>\n",
       "      <th>OTHER</th>\n",
       "      <td>522</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Ethnic group</th>\n",
       "      <th>OTHER</th>\n",
       "      <td>4355</td>\n",
       "      <td>4355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEL</th>\n",
       "      <th>AMR</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUR</th>\n",
       "      <th>AMR</th>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pakistani</th>\n",
       "      <th>ASIAN</th>\n",
       "      <td>1747</td>\n",
       "      <td>1747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prefer not to answer</th>\n",
       "      <th>OTHER</th>\n",
       "      <td>1583</td>\n",
       "      <td>1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSI</th>\n",
       "      <th>EUR</th>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <th>WHITE</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White and  Black African</th>\n",
       "      <th>MIXED</th>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White and Asian</th>\n",
       "      <th>MIXED</th>\n",
       "      <td>802</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White and Black carribean</th>\n",
       "      <th>MIXED</th>\n",
       "      <td>597</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YRI</th>\n",
       "      <th>AFR</th>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       FID     IID\n",
       "Ethnicity                  group                  \n",
       "ACB                        AFR         102     102\n",
       "ASW                        AFR         104     104\n",
       "African                    BLACK      3205    3205\n",
       "Any other Asian background ASIAN      1747    1747\n",
       "Any other Black background BLACK       118     118\n",
       "Any other Mixed background MIXED       996     996\n",
       "Any other white Background WHITE     15760   15760\n",
       "Asian or Asian British     ASIAN        42      42\n",
       "Bangladeshi                ASIAN       221     221\n",
       "Black or Black Britsh      BLACK        26      26\n",
       "British                    WHITE    430307  430307\n",
       "CDX                        EAS          98      98\n",
       "CEU                        EUR         183     183\n",
       "CHB                        EAS         108     108\n",
       "CHD                        EAS           1       1\n",
       "CHS                        EAS         153     153\n",
       "CLM                        AMR         107     107\n",
       "Caribbean                  BLACK      4299    4299\n",
       "Chinese                    CHINESE    1503    1503\n",
       "Do notknow                 OTHER       204     204\n",
       "FIN                        EUR         100     100\n",
       "GBR                        EUR         104     104\n",
       "GIH                        SAS         113     113\n",
       "IBS                        EUR         150     150\n",
       "Indian                     ASIAN      5660    5660\n",
       "Irish                      WHITE     12719   12719\n",
       "JPT                        EAS         105     105\n",
       "KHV                        SAS         121     121\n",
       "LWK                        AFR         116     116\n",
       "MKK                        AFR          31      31\n",
       "MXL                        AMR         103     103\n",
       "Mixed                      MIXED        46      46\n",
       "No Ethnicity               OTHER       522     522\n",
       "Other Ethnic group         OTHER      4355    4355\n",
       "PEL                        AMR         105     105\n",
       "PUR                        AMR         111     111\n",
       "Pakistani                  ASIAN      1747    1747\n",
       "Prefer not to answer       OTHER      1583    1583\n",
       "TSI                        EUR         112     112\n",
       "White                      WHITE       545     545\n",
       "White and  Black African   MIXED       402     402\n",
       "White and Asian            MIXED       802     802\n",
       "White and Black carribean  MIXED       597     597\n",
       "YRI                        AFR         189     189"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auxiliary data maps ethnicity to index.\n",
    "aux_data_df.groupby(['Ethnicity','group']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use parent groupings in current data\n",
    "ethnic_families = {}\n",
    "#set(aux_data_df['Ethnicity'].values.tolist())\n",
    "\n",
    "for item in aux_data_df[['Ethnicity','group']].drop_duplicates().values.tolist():\n",
    "    try:\n",
    "        ethnic_families[item[1]].append(item[0].strip())\n",
    "    except KeyError:\n",
    "        ethnic_families[item[1]] = [item[0]]\n",
    "        \n",
    "    #try: \n",
    "    #    pop_by_continent[split_line[2]].append(split_line[1])\n",
    "    #except KeyError:\n",
    "    #    pop_by_continent[split_line[2]] = [split_line[1]]\n",
    "\n",
    "#for item in set(aux_data_df['Ethnicity'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHITE 4\n",
      "ASIAN 5\n",
      "BLACK 4\n",
      "OTHER 4\n",
      "MIXED 5\n",
      "CHINESE 1\n",
      "EUR 5\n",
      "EAS 5\n",
      "AMR 4\n",
      "AFR 5\n",
      "SAS 2\n",
      "\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for key in ethnic_families.keys():\n",
    "    print(key, len(ethnic_families[key]))\n",
    "\n",
    "print()\n",
    "print(len(ethnic_families.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFR': ['YRI', 'LWK', 'ASW', 'MKK', 'ACB'],\n",
       " 'AMR': ['PUR', 'CLM', 'MXL', 'PEL'],\n",
       " 'ASIAN': ['Pakistani',\n",
       "  'Indian',\n",
       "  'Any other Asian background',\n",
       "  'Asian or Asian British',\n",
       "  'Bangladeshi'],\n",
       " 'BLACK': ['African',\n",
       "  'Caribbean',\n",
       "  'Any other Black background',\n",
       "  'Black or Black Britsh'],\n",
       " 'CHINESE': ['Chinese'],\n",
       " 'EAS': ['CHS', 'CHB', 'CHD', 'JPT', 'CDX'],\n",
       " 'EUR': ['GBR', 'FIN', 'IBS', 'CEU', 'TSI'],\n",
       " 'MIXED': ['White and Black carribean',\n",
       "  'White and  Black African',\n",
       "  'White and Asian',\n",
       "  'Any other Mixed background',\n",
       "  'Mixed'],\n",
       " 'OTHER': ['Other Ethnic group',\n",
       "  'Prefer not to answer',\n",
       "  'Do notknow',\n",
       "  'No Ethnicity'],\n",
       " 'SAS': ['GIH', 'KHV'],\n",
       " 'WHITE': ['British', 'Any other white Background', 'Irish', 'White']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnic_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find a colour family for each of these:\n",
    "#WHITE 4 Category20c[4][-4:] (blue)\n",
    "#ASIAN 5 BuPu[9][0:4] (purple)\n",
    "#BLACK 4 Category20c[8][-4:] (orange)\n",
    "#OTHER 4 Category20c[-4:] (gray)\n",
    "#MIXED 5 BuGn[0:4] (green)\n",
    "#CHINESE 1 Purples[9][5] (purple)\n",
    "#EUR 5 PuRd[9][0:4] (red-pink)\n",
    "#EAS (east asian) 5 Purples[9][0:4]\n",
    "#AMR (Americas) 4 Category20b[12][-4:]\n",
    "#AFR (Africa) 5 OrRd[9][0:4] (red-Orange)\n",
    "#SAS 2 Category20b[20] Category20b[14][-2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.palettes import Category20b, Category20c, BuPu, BuGn, Purples, PuRd, OrRd, Set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_dict = {}\n",
    "for key in ethnic_families:\n",
    "    if key == 'WHITE':\n",
    "        color_dict[key]=Category20c[4][-4:]\n",
    "    elif key == 'ASIAN':\n",
    "        color_dict[key]=BuPu[9][0:5]\n",
    "    elif key == 'BLACK':\n",
    "        color_dict[key]=Category20c[8][-4:]\n",
    "    elif key == 'OTHER':\n",
    "        color_dict[key]=Category20c[20][-4:]\n",
    "    elif key == 'MIXED':\n",
    "        color_dict[key]=BuGn[9][-5:]\n",
    "    elif key == 'CHINESE':\n",
    "        color_dict[key]=[Purples[9][5]]\n",
    "    elif key == 'EUR':\n",
    "        color_dict[key]=PuRd[9][0:5]\n",
    "    elif key == 'EAS':\n",
    "        color_dict[key]=Purples[9][0:5]\n",
    "    elif key == 'AMR':\n",
    "        color_dict[key]=Category20b[12][-4:]\n",
    "    elif key == 'AFR':\n",
    "        color_dict[key]=OrRd[9][0:5]\n",
    "    elif key == 'SAS':\n",
    "        color_dict[key]=Category20b[14][-2:]\n",
    "    \n",
    "for key in ethnic_families:\n",
    "    if len(ethnic_families[key])-len(color_dict[key])!=0:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFR': ['#7f0000', '#b30000', '#d7301f', '#ef6548', '#fc8d59'],\n",
       " 'AMR': ['#8c6d31', '#bd9e39', '#e7ba52', '#e7cb94'],\n",
       " 'ASIAN': ['#4d004b', '#810f7c', '#88419d', '#8c6bb1', '#8c96c6'],\n",
       " 'BLACK': ['#e6550d', '#fd8d3c', '#fdae6b', '#fdd0a2'],\n",
       " 'CHINESE': ['#bcbddc'],\n",
       " 'EAS': ['#3f007d', '#54278f', '#6a51a3', '#807dba', '#9e9ac8'],\n",
       " 'EUR': ['#67001f', '#980043', '#ce1256', '#e7298a', '#df65b0'],\n",
       " 'MIXED': ['#66c2a4', '#99d8c9', '#ccece6', '#e5f5f9', '#f7fcfd'],\n",
       " 'OTHER': ['#636363', '#969696', '#bdbdbd', '#d9d9d9'],\n",
       " 'SAS': ['#843c39', '#ad494a'],\n",
       " 'WHITE': ['#3182bd', '#6baed6', '#9ecae1', '#c6dbef']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Legend labels\n",
    "label_dict = {'British':'British',\n",
    "'Any other white Background':'Any other white Background',\n",
    "'Pakistani':'Pakistani',\n",
    "'African':'African',\n",
    "'Irish':'Irish',\n",
    "'Other Ethnic group':'Other Ethnic group',\n",
    "'Caribbean':'Caribbean',\n",
    "'Indian':'Indian',\n",
    "'Prefer not to answer':'Prefer not to answer',\n",
    "'Any other Asian background':'Any other Asian background',\n",
    "'White':'White',\n",
    "'White and Black carribean':'White and Black carribean',\n",
    "'Asian or Asian British':'Asian or Asian British',\n",
    "'Any other Black background':'Any other Black background',\n",
    "'Do notknow':'Do not know',\n",
    "'Chinese':'Chinese (UKBB)',\n",
    "'White and  Black African':'White and Black',\n",
    "'White and Asian':'White and Asian',\n",
    "'Any other Mixed background':'Any Other Mixed',\n",
    "'No Ethnicity':'No Ethnicity',\n",
    "'Bangladeshi':'Bangladeshi (UKBB)',\n",
    "'Black or Black Britsh':'Black or Black British',\n",
    "'Mixed':'Mixed',\n",
    "'CDX':'Chinese Dai in Xishuangbanna, China',\n",
    "'CHB':'Han Chinese in Bejing, China',\n",
    "'JPT':'Japanese in Tokyo, Japan',\n",
    "'KHV':'Kinh in Ho Chi Minh City, Vietnam',\n",
    "'CHS':'Southern Han Chinese, China',\n",
    "'BEB':'Bengali in Bangladesh',\n",
    "'GIH':'Gujarati Indian in Houston,TX',\n",
    "'ITU':'Indian Telugu in the UK',\n",
    "'PJL':'Punjabi in Lahore,Pakistan',\n",
    "'STU':'Sri Lankan Tamil in the UK',\n",
    "'ASW':'African Ancestry in Southwest US',\n",
    "'ACB':'African Caribbean in Barbados',\n",
    "'ESN':'Esan in Nigeria',\n",
    "'GWD':'Gambian in Western Division',\n",
    "'LWK':'Luhya in Webuye, Kenya',\n",
    "'MSL':'Mende in Sierra Leone',\n",
    "'YRI':'Yoruba in Ibadan, Nigeria',\n",
    "'GBR':'British in England and Scotland',\n",
    "'FIN':'Finnish in Finland',\n",
    "'IBS':'Iberian populations in Spain',\n",
    "'TSI':'Toscani in Italy',\n",
    "'CEU':'Utah residents with N and W European ancestry',\n",
    "'CLM':'Colombian in Medellin, Colombia',\n",
    "'MXL':'Mexican Ancestry in LA, California',\n",
    "'PEL':'Peruvian in Lima, Peru',\n",
    "'PUR':'Puerto Rican in Puerto Rico',\n",
    "'CHD':'CHD',\n",
    "'MKK':'MKK'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marker_dict_text = {'British':'BRI',\n",
    "'Any other white Background':'AOW',\n",
    "'Pakistani':'PKS',\n",
    "'African':'AFR',\n",
    "'Irish':'IRE',\n",
    "'Other Ethnic group':'OEG',\n",
    "'Caribbean':'CAR',\n",
    "'Indian':'IND_UKBB',\n",
    "'Prefer not to answer':'PNA',\n",
    "'Any other Asian background':'AOA',\n",
    "'White':'WHI',\n",
    "'White and Black carribean':'WAB',\n",
    "'Asian or Asian British':'AAB',\n",
    "'Any other Black background':'AOB',\n",
    "'Do notknow':'DNK',\n",
    "'Chinese':'CHI_U',\n",
    "'White and  Black African':'WAB',\n",
    "'White and Asian':'WAA',\n",
    "'Any other Mixed background':'AOM',\n",
    "'No Ethnicity':'NE',\n",
    "'Bangladeshi':'BGL',\n",
    "'Black or Black Britsh':'BBB',\n",
    "'Mixed':'MIX',\n",
    "'CDX':'CDX',\n",
    "'CHB':'CHB',\n",
    "'JPT':'JPT',\n",
    "'KHV':'KHV',\n",
    "'CHS':'CHS',\n",
    "'BEB':'BEB',\n",
    "'GIH':'GIH',\n",
    "'ITU':'ITU',\n",
    "'PJL':'PJL',\n",
    "'STU':'STU',\n",
    "'ASW':'ASW',\n",
    "'ACB':'ACB',\n",
    "'ESN':'ESN',\n",
    "'GWD':'GWD',\n",
    "'LWK':'LWK',\n",
    "'MSL':'MSL',\n",
    "'YRI':'YRI',\n",
    "'GBR':'GBR',\n",
    "'FIN':'FIN',\n",
    "'IBS':'IBS',\n",
    "'TSI':'TSI',\n",
    "'CEU':'CEU',\n",
    "'CLM':'CLM',\n",
    "'MXL':'MXL',\n",
    "'PEL':'PEL',\n",
    "'PUR':'PUR',\n",
    "'CHD':'CHD',\n",
    "'MKK':'MKK'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionaries, populations, and colours have been defined\n",
    "# Now need to get the indices of each population\n",
    "indices_of_population = defaultdict(list)\n",
    "ethnicity_list = aux_data_df['Ethnicity'].values.tolist()\n",
    "\n",
    "for a in range(0, len(ethnicity_list)):\n",
    "    try:\n",
    "        indices_of_population[ethnicity_list[a]].append(a)\n",
    "    except KeyError:\n",
    "        print('missing individual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British\n",
      "Any other white Background\n",
      "Irish\n",
      "White\n",
      "Pakistani\n",
      "Indian\n",
      "Any other Asian background\n",
      "Asian or Asian British\n",
      "Bangladeshi\n",
      "African\n",
      "Caribbean\n",
      "Any other Black background\n",
      "Black or Black Britsh\n",
      "Other Ethnic group\n",
      "Prefer not to answer\n",
      "Do notknow\n",
      "No Ethnicity\n",
      "White and Black carribean\n",
      "White and  Black African\n",
      "White and Asian\n",
      "Any other Mixed background\n",
      "Mixed\n",
      "Chinese\n",
      "GBR\n",
      "FIN\n",
      "IBS\n",
      "CEU\n",
      "TSI\n",
      "CHS\n",
      "CHB\n",
      "CHD\n",
      "JPT\n",
      "CDX\n",
      "PUR\n",
      "CLM\n",
      "MXL\n",
      "PEL\n",
      "YRI\n",
      "LWK\n",
      "ASW\n",
      "MKK\n",
      "ACB\n",
      "GIH\n",
      "KHV\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "ax = fig.add_subplot(111, aspect=1)\n",
    "\n",
    "for fam in ethnic_families.keys():\n",
    "    cc = 0 # Colouring count\n",
    "    for eth in ethnic_families[fam]:\n",
    "        if eth=='Irish':\n",
    "            temp_colour=Set1[6][-1]\n",
    "        else:\n",
    "            temp_colour=color_dict[fam][cc]\n",
    "            \n",
    "        temp_proj = ukbb_1000g[indices_of_population[eth]]\n",
    "        ax.scatter(temp_proj[:,0],temp_proj[:,1],label=label_dict[eth],color=temp_colour,s=20,alpha=0)\n",
    "        cc+=1\n",
    "        \n",
    "        print(eth)\n",
    "        \n",
    "        # Note: Generate geographic plots annotated with truncated Northing/Easting values.\n",
    "        for i in range(0, temp_proj.shape[0]):\n",
    "            ax.annotate(marker_dict_text[eth], xy=temp_proj[i], color=temp_colour, alpha=1,\n",
    "                       fontsize=10)\n",
    "        #ax.annotate(marker_dict[eth])\n",
    "\n",
    "ax.legend(ncol=5,loc='lower center', bbox_to_anchor=(0.4,-0.12), fontsize=40,markerscale=25)\n",
    "\n",
    "fig.savefig('test.jpeg',format='jpeg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['British', 'Any other white Background', 'Irish', 'White']\n",
      "['Pakistani', 'Indian', 'Any other Asian background', 'Asian or Asian British', 'Bangladeshi']\n",
      "['African', 'Caribbean', 'Any other Black background', 'Black or Black Britsh']\n",
      "['Other Ethnic group', 'Prefer not to answer', 'Do notknow', 'No Ethnicity']\n",
      "['White and Black carribean', 'White and  Black African', 'White and Asian', 'Any other Mixed background', 'Mixed']\n",
      "['Chinese']\n",
      "['GBR', 'FIN', 'IBS', 'CEU', 'TSI']\n",
      "['CHS', 'CHB', 'CHD', 'JPT', 'CDX']\n",
      "['PUR', 'CLM', 'MXL', 'PEL']\n",
      "['YRI', 'LWK', 'ASW', 'MKK', 'ACB']\n",
      "['GIH', 'KHV']\n"
     ]
    }
   ],
   "source": [
    "# Create a legible label ()\n",
    "# Create a legible\n",
    "for key in ethnic_families.keys():\n",
    "    print(ethnic_families[key])\n",
    "    \n",
    "# Translate the 1000G codes to text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
